{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, log_loss, accuracy_score, make_scorer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers import  Dropout, Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_binary_encoding = pd.read_csv('../../Feature_Encoding/data/train_binary_encoding.csv')\n",
    "df_test_binary_encoding = pd.read_csv('../../Feature_Encoding/data/test_binary_encoding.csv')\n",
    "train = pd.read_csv('../../Feature_Engineering/data/other-cleaned_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model, x_train, y_train):\n",
    "    score_cross_val = model_selection.cross_val_score(model, x_train, y_train, cv=5)\n",
    "    print(score_cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal profunda - https://keras.io/guides/sequential_model/\n",
    "# nFeatures -> cantidad de features (columnas de set)\n",
    "# nClasses -> cantidad de clases que pueden ser (2 -> lost vs won)\n",
    "def DNN_model_bin(optimizer='rmsprop',init='glorot_uniform'):\n",
    "    node = 512\n",
    "    nClasses = 2\n",
    "    dropout=0.5\n",
    "    nFeatures = 185\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(node,input_dim=nFeatures,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,4):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "# Red neuronal profunda - https://keras.io/guides/sequential_model/\n",
    "# nFeatures -> cantidad de features (columnas de set)\n",
    "# nClasses -> cantidad de clases que pueden ser (2 -> lost vs won)\n",
    "def DNN_model(optimizer='rmsprop',init='glorot_uniform'):\n",
    "    node = 512\n",
    "    nClasses = 2\n",
    "    dropout=0.5\n",
    "    nFeatures = 55\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(node,input_dim=nFeatures,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,4):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(x_train, y_train, x_validation, y_validation):\n",
    "    keras_model = KerasClassifier(build_fn=DNN_model)\n",
    "    optimizers = ['rmsprop', 'adam']\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    epochs = [50, 100, 150]\n",
    "    batches = batches = [5, 10, 20]\n",
    "    param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init)\n",
    "    dnn_gs = GridSearchCV(keras_model, param_grid=param_grid)\n",
    "    dnn_gs.fit(x_train, y_train)\n",
    "    dnn_best = dnn_gs.best_estimator_\n",
    "    print(dnn_gs.best_params_)\n",
    "    print('dnn: {}'.format(dnn_best.score(x_validation, y_validation)))\n",
    "    return dnn_best\n",
    "\n",
    "def dnn_bin(x_train, y_train, x_validation, y_validation):\n",
    "    keras_model = KerasClassifier(build_fn=DNN_model_bin)\n",
    "    optimizers = ['rmsprop', 'adam']\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    epochs = [50, 100, 150]\n",
    "    batches = batches = [5, 10, 20]\n",
    "    param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init)\n",
    "    dnn_gs = GridSearchCV(keras_model, param_grid=param_grid)\n",
    "    dnn_gs.fit(x_train, y_train)\n",
    "    dnn_best = dnn_gs.best_estimator_\n",
    "    print(dnn_gs.best_params_)\n",
    "    print('dnn: {}'.format(dnn_best.score(x_validation, y_validation)))\n",
    "    return dnn_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x_test, y_test):\n",
    "    predictions = model.predict_proba(x_test)[:,1]\n",
    "    logloss = log_loss(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions.round())\n",
    "    print(\"Accuracy: %.2f%%, Logloss: %.2f\" % (accuracy*100.0, logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.Target\n",
    "x_train_binary_encoding, x_validation_binary_encoding, y_train_binary_encoding, y_validation_binary_encoding = train_test_split(df_train_binary_encoding, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891/1891 [==============================] - 40s 21ms/step - loss: 53655.0070 - accuracy: 0.5159\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 1048.4277 - accuracy: 0.6497\n",
      "1891/1891 [==============================] - 40s 20ms/step - loss: 94315.5605 - accuracy: 0.5137\n",
      "473/473 [==============================] - 2s 4ms/step - loss: 1261.3635 - accuracy: 0.5753\n",
      "1891/1891 [==============================] - 35s 18ms/step - loss: 44443.8826 - accuracy: 0.5067\n",
      "473/473 [==============================] - 2s 4ms/step - loss: 55.5317 - accuracy: 0.5973\n",
      "1891/1891 [==============================] - 34s 18ms/step - loss: 52963.1255 - accuracy: 0.5298\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 6668.5679 - accuracy: 0.5599\n",
      "1891/1891 [==============================] - 41s 21ms/step - loss: 51968.1891 - accuracy: 0.5173\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 731.9033 - accuracy: 0.6839\n",
      "1891/1891 [==============================] - 35s 18ms/step - loss: 61870.0925 - accuracy: 0.5331\n",
      "473/473 [==============================] - 2s 4ms/step - loss: 940.5571 - accuracy: 0.6654\n",
      "1891/1891 [==============================] - 36s 18ms/step - loss: 56593.9683 - accuracy: 0.5246\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 3714.6843 - accuracy: 0.5558\n",
      "1891/1891 [==============================] - 37s 19ms/step - loss: 46516.2630 - accuracy: 0.5162\n",
      "473/473 [==============================] - 2s 5ms/step - loss: 1491.6687 - accuracy: 0.6324\n",
      "1891/1891 [==============================] - 38s 20ms/step - loss: 53388.0836 - accuracy: 0.5222\n",
      "473/473 [==============================] - 4s 9ms/step - loss: 1650.6095 - accuracy: 0.6509\n",
      "1891/1891 [==============================] - 70s 37ms/step - loss: 51503.6691 - accuracy: 0.5223\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 2118.7817 - accuracy: 0.5620\n",
      "1891/1891 [==============================] - 65s 34ms/step - loss: 45587.4442 - accuracy: 0.5269\n",
      "473/473 [==============================] - 4s 7ms/step - loss: 6809.7681 - accuracy: 0.4374\n",
      "1891/1891 [==============================] - 63s 33ms/step - loss: 66137.7890 - accuracy: 0.5156\n",
      "473/473 [==============================] - 4s 7ms/step - loss: 848.8765 - accuracy: 0.6358\n",
      "1891/1891 [==============================] - 63s 32ms/step - loss: 75316.7857 - accuracy: 0.5234\n",
      "473/473 [==============================] - 3s 7ms/step - loss: 104.5552 - accuracy: 0.6679\n",
      "1891/1891 [==============================] - 61s 32ms/step - loss: 62504.1319 - accuracy: 0.5234\n",
      "473/473 [==============================] - 4s 7ms/step - loss: 3108.9744 - accuracy: 0.5781\n",
      "1891/1891 [==============================] - 61s 32ms/step - loss: 81908.1166 - accuracy: 0.5131\n",
      "473/473 [==============================] - 4s 8ms/step - loss: 2045.8265 - accuracy: 0.5628\n",
      "1891/1891 [==============================] - 61s 32ms/step - loss: 64169.2327 - accuracy: 0.5406\n",
      "473/473 [==============================] - 3s 7ms/step - loss: 16436.5742 - accuracy: 0.5626\n",
      "1891/1891 [==============================] - 62s 32ms/step - loss: 87374.1686 - accuracy: 0.5199\n",
      "473/473 [==============================] - 3s 7ms/step - loss: 3715.1113 - accuracy: 0.4636\n",
      "1891/1891 [==============================] - 63s 33ms/step - loss: 38683.8441 - accuracy: 0.5174\n",
      "473/473 [==============================] - 4s 7ms/step - loss: 485.7586 - accuracy: 0.6396\n",
      "1891/1891 [==============================] - 58s 30ms/step - loss: 43429.2778 - accuracy: 0.5259\n",
      "473/473 [==============================] - 3s 7ms/step - loss: 1389.9968 - accuracy: 0.6166\n",
      "1891/1891 [==============================] - 57s 30ms/step - loss: 113245.9958 - accuracy: 0.5040\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 530.4720 - accuracy: 0.6822\n",
      "1891/1891 [==============================] - 57s 30ms/step - loss: 51494.6445 - accuracy: 0.5226\n",
      "473/473 [==============================] - 3s 7ms/step - loss: 1497.9015 - accuracy: 0.6324\n",
      "1891/1891 [==============================] - 59s 31ms/step - loss: 73745.2287 - accuracy: 0.5141\n",
      "473/473 [==============================] - 3s 7ms/step - loss: 4565.1309 - accuracy: 0.6641\n",
      "1891/1891 [==============================] - 62s 32ms/step - loss: 61992.7539 - accuracy: 0.5269\n",
      "473/473 [==============================] - 3s 7ms/step - loss: 343.1171 - accuracy: 0.5821\n",
      "1891/1891 [==============================] - 58s 30ms/step - loss: 118418.4847 - accuracy: 0.5278\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 5582.8926 - accuracy: 0.6411\n",
      "1891/1891 [==============================] - 56s 29ms/step - loss: 43042.7806 - accuracy: 0.5272\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 4156.9365 - accuracy: 0.5959\n",
      "1891/1891 [==============================] - 58s 30ms/step - loss: 73080.5721 - accuracy: 0.5191\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 226.6239 - accuracy: 0.6650\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 69327.0375 - accuracy: 0.5382\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 2469.6638 - accuracy: 0.6180\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 55019.8720 - accuracy: 0.5262\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 351.4544 - accuracy: 0.6662\n",
      "1891/1891 [==============================] - 51s 26ms/step - loss: 60817.7332 - accuracy: 0.5248\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 694.5371 - accuracy: 0.4774\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 62467.3682 - accuracy: 0.5259\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 4043.6013 - accuracy: 0.5628\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 39605.3339 - accuracy: 0.5318\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1480.7434 - accuracy: 0.6290\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 50956.6593 - accuracy: 0.5221\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 408.6017 - accuracy: 0.6341\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 65234.3667 - accuracy: 0.5257\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 10336.2207 - accuracy: 0.4179\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 72556.6624 - accuracy: 0.5214\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1121.6102 - accuracy: 0.6314A: 1s - loss: 1274.909\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 60259.1319 - accuracy: 0.5181\n",
      "473/473 [==============================] - 4s 7ms/step - loss: 4337.4478 - accuracy: 0.5628\n",
      "1891/1891 [==============================] - 59s 31ms/step - loss: 69608.8879 - accuracy: 0.5151\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 5466.8281 - accuracy: 0.5893\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 64840.2949 - accuracy: 0.5144\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 3640.3008 - accuracy: 0.6481\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 83690.2100 - accuracy: 0.5225\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 6392.2012 - accuracy: 0.5821\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 43127.9038 - accuracy: 0.5167\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 2830.3220 - accuracy: 0.5370\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 77788.6206 - accuracy: 0.5207\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 461.4706 - accuracy: 0.6923\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 57837.4492 - accuracy: 0.5157\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 3754.0007 - accuracy: 0.5656\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 76592.3498 - accuracy: 0.5334\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 6341.1782 - accuracy: 0.5558\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 63681.2097 - accuracy: 0.5214\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 382.0668 - accuracy: 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891/1891 [==============================] - 52s 27ms/step - loss: 69796.2320 - accuracy: 0.5361\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 462.8992 - accuracy: 0.6471\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 59591.9581 - accuracy: 0.5228\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1218.0516 - accuracy: 0.6030\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 68671.4275 - accuracy: 0.5223\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 3999.9556 - accuracy: 0.5410\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 55875.8314 - accuracy: 0.5334\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 3340.8921 - accuracy: 0.6332\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 49993.0066 - accuracy: 0.5239\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 339.4034 - accuracy: 0.6637\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 60505.4380 - accuracy: 0.5316\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 143.7705 - accuracy: 0.6623\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 52299.9351 - accuracy: 0.5170\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 457.6145 - accuracy: 0.6936\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 56954.3774 - accuracy: 0.5195\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1808.2876 - accuracy: 0.6019\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 52309.0847 - accuracy: 0.5236\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 1635.6943 - accuracy: 0.6316\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 79288.7364 - accuracy: 0.5319\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 741.0868 - accuracy: 0.6701\n",
      "1891/1891 [==============================] - 51s 26ms/step - loss: 56283.0581 - accuracy: 0.5193\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 2590.4094 - accuracy: 0.5599\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 37822.9222 - accuracy: 0.4967\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 1485.7206 - accuracy: 0.4909\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 215628.7429 - accuracy: 0.5133\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 11586.1992 - accuracy: 0.5626\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 58384.5662 - accuracy: 0.5402\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1572.5858 - accuracy: 0.4442\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 71813.5132 - accuracy: 0.5306\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1179.2246 - accuracy: 0.6180\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 52871.4805 - accuracy: 0.5230\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 3350.0989 - accuracy: 0.5599\n",
      "1891/1891 [==============================] - 51s 26ms/step - loss: 50516.8144 - accuracy: 0.5160\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 6813.9048 - accuracy: 0.5628\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 61248.5849 - accuracy: 0.5229\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 2076.2012 - accuracy: 0.4374\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 53306.7916 - accuracy: 0.5095\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 89.0465 - accuracy: 0.5558\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 86125.3545 - accuracy: 0.5264\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1350.8896 - accuracy: 0.5842\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 55263.7435 - accuracy: 0.5177\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 474.2013 - accuracy: 0.6619\n",
      "1891/1891 [==============================] - 51s 26ms/step - loss: 92673.7974 - accuracy: 0.5270\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 543.1989 - accuracy: 0.6521\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 71896.5825 - accuracy: 0.5216\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 3538.0994 - accuracy: 0.5626\n",
      "1891/1891 [==============================] - 56s 29ms/step - loss: 49321.0290 - accuracy: 0.5221\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 255.4279 - accuracy: 0.6629\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 121341.8984 - accuracy: 0.5180\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1432.0178 - accuracy: 0.6641\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 58007.3146 - accuracy: 0.5318\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 2532.0796 - accuracy: 0.5853\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 53724.3858 - accuracy: 0.5327\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 1837.8265 - accuracy: 0.5628\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 45359.1391 - accuracy: 0.5134\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 89.3499 - accuracy: 0.5626\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 52071.8716 - accuracy: 0.5429\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1224.5607 - accuracy: 0.5558\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 50123.2983 - accuracy: 0.5328\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 8631.4111 - accuracy: 0.5821\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 58984.8685 - accuracy: 0.5057\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 707.2101 - accuracy: 0.6746\n",
      "1891/1891 [==============================] - 50s 26ms/step - loss: 164820.9213 - accuracy: 0.5225\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 7524.7568 - accuracy: 0.6267\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 67706.2002 - accuracy: 0.5243\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 769.5173 - accuracy: 0.6591\n",
      "1891/1891 [==============================] - 54s 28ms/step - loss: 53341.7889 - accuracy: 0.5142\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 2337.7290 - accuracy: 0.6155\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 46028.5955 - accuracy: 0.5188\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 8703.7041 - accuracy: 0.5821\n",
      "1891/1891 [==============================] - 51s 26ms/step - loss: 69104.0178 - accuracy: 0.5254\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1423.3926 - accuracy: 0.5573\n",
      "1891/1891 [==============================] - 51s 26ms/step - loss: 63954.5216 - accuracy: 0.5165\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 1297.5765 - accuracy: 0.5997\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 55661.4404 - accuracy: 0.5157\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 1224.8396 - accuracy: 0.6269\n",
      "1891/1891 [==============================] - 53s 27ms/step - loss: 63409.4076 - accuracy: 0.5318\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 172.5894 - accuracy: 0.6502\n",
      "1891/1891 [==============================] - 53s 28ms/step - loss: 43462.5985 - accuracy: 0.5266\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 6247.1279 - accuracy: 0.5821\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 51184.3397 - accuracy: 0.5326\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 234.1853 - accuracy: 0.5637\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 74097.8217 - accuracy: 0.5110\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 481.4124 - accuracy: 0.6077\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 57785.1629 - accuracy: 0.5241\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 1252.8136 - accuracy: 0.5626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891/1891 [==============================] - 55s 29ms/step - loss: 49523.2083 - accuracy: 0.5177\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 3672.3887 - accuracy: 0.5558\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 50139.4633 - accuracy: 0.5124\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 2618.3425 - accuracy: 0.5563\n",
      "1891/1891 [==============================] - 52s 27ms/step - loss: 101234.3092 - accuracy: 0.5079\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 2953.5251 - accuracy: 0.6111\n",
      "1891/1891 [==============================] - 51s 27ms/step - loss: 88207.3954 - accuracy: 0.5311\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 13420.1904 - accuracy: 0.5628\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 42963.5148 - accuracy: 0.5141\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 1544.5939 - accuracy: 0.5626\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 58098.8972 - accuracy: 0.5172\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 5024.4023 - accuracy: 0.5558\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 58780.4645 - accuracy: 0.5187\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3492.2712 - accuracy: 0.5025\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 58359.0144 - accuracy: 0.5342\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 2333.7708 - accuracy: 0.5599\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 75141.0174 - accuracy: 0.5199\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 10365.3682 - accuracy: 0.5628\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 59009.2963 - accuracy: 0.5232\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 27825.0117 - accuracy: 0.5626\n",
      "946/946 [==============================] - 28s 29ms/step - loss: 49658.4211 - accuracy: 0.5258\n",
      "237/237 [==============================] - ETA: 0s - loss: 1297.8101 - accuracy: 0.579 - 2s 6ms/step - loss: 1366.8881 - accuracy: 0.5787\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 53297.7679 - accuracy: 0.5160\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 10703.5186 - accuracy: 0.5863\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 63381.5080 - accuracy: 0.5135\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 4075.9849 - accuracy: 0.5188\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 79633.3840 - accuracy: 0.5097\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1918.4922 - accuracy: 0.6204\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 50213.2150 - accuracy: 0.5223\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 9336.8223 - accuracy: 0.5626\n",
      "946/946 [==============================] - 28s 29ms/step - loss: 70467.5083 - accuracy: 0.5075\n",
      "237/237 [==============================] - 1s 6ms/step - loss: 878.0117 - accuracy: 0.5558\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 121609.5660 - accuracy: 0.5065\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1733.5278 - accuracy: 0.6684\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 46260.7160 - accuracy: 0.5254\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 4967.5972 - accuracy: 0.6035\n",
      "946/946 [==============================] - 32s 32ms/step - loss: 57380.5541 - accuracy: 0.5193\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 618.9288 - accuracy: 0.6898\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 85892.5387 - accuracy: 0.5142\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1694.5891 - accuracy: 0.6277\n",
      "946/946 [==============================] - 31s 31ms/step - loss: 45322.3280 - accuracy: 0.5120\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3566.1531 - accuracy: 0.5558\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 47692.3471 - accuracy: 0.5180\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 1857.2671 - accuracy: 0.6688\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 73557.6710 - accuracy: 0.5128\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 7835.4287 - accuracy: 0.6293\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 59957.5809 - accuracy: 0.5177\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1929.9731 - accuracy: 0.6314\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 54775.5647 - accuracy: 0.5186\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3640.1443 - accuracy: 0.4391\n",
      "946/946 [==============================] - 28s 28ms/step - loss: 81554.6464 - accuracy: 0.5270\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 314.8359 - accuracy: 0.5558\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 50339.3223 - accuracy: 0.5259\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 2745.7297 - accuracy: 0.6646\n",
      "946/946 [==============================] - 28s 29ms/step - loss: 44702.1243 - accuracy: 0.5140\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 5244.9604 - accuracy: 0.5599\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 60357.6338 - accuracy: 0.5157\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 742.1243 - accuracy: 0.5628\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 44305.5080 - accuracy: 0.5229\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 6863.1797 - accuracy: 0.5537\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 48682.2900 - accuracy: 0.5116\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3224.0464 - accuracy: 0.6121\n",
      "946/946 [==============================] - 28s 29ms/step - loss: 69219.1676 - accuracy: 0.5073\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2279.8145 - accuracy: 0.5778\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 57771.4941 - accuracy: 0.5160\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 13394.3584 - accuracy: 0.4401\n",
      "946/946 [==============================] - 28s 29ms/step - loss: 80229.0497 - accuracy: 0.5168\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 11985.5537 - accuracy: 0.5628\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 61447.6643 - accuracy: 0.5131\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 17318.6504 - accuracy: 0.5711\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 53574.5490 - accuracy: 0.5268\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1861.5875 - accuracy: 0.6675\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 110313.7212 - accuracy: 0.5243\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 5520.4624 - accuracy: 0.5821\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 66393.9340 - accuracy: 0.5200\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1957.3027 - accuracy: 0.6301\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 51716.5420 - accuracy: 0.5222\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1694.0724 - accuracy: 0.6847\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 55979.2224 - accuracy: 0.5222\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 362.0100 - accuracy: 0.5334\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 55950.5224 - accuracy: 0.5052\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1508.1829 - accuracy: 0.6789\n",
      "946/946 [==============================] - 28s 28ms/step - loss: 58582.4921 - accuracy: 0.4993\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 4384.5566 - accuracy: 0.5821\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 55198.4827 - accuracy: 0.5169\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 4434.8540 - accuracy: 0.6136\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 71969.3240 - accuracy: 0.5009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - 2s 6ms/step - loss: 2970.7412 - accuracy: 0.5578\n",
      "946/946 [==============================] - 28s 29ms/step - loss: 93277.5083 - accuracy: 0.5174\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 4484.5840 - accuracy: 0.6290\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 75025.7304 - accuracy: 0.5247\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 136.0585 - accuracy: 0.6299\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 54642.0897 - accuracy: 0.5232\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 2671.0122 - accuracy: 0.6366\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 71351.4644 - accuracy: 0.5224\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 6404.3335 - accuracy: 0.5599\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 53066.6208 - accuracy: 0.5318\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 13199.7354 - accuracy: 0.5628\n",
      "946/946 [==============================] - 28s 29ms/step - loss: 50368.0443 - accuracy: 0.5185\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1420.5806 - accuracy: 0.5402\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 63723.9313 - accuracy: 0.5271\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 7085.1055 - accuracy: 0.5618\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 44848.3090 - accuracy: 0.5200\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 624.3372 - accuracy: 0.6172\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 70757.4694 - accuracy: 0.5184\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3816.1143 - accuracy: 0.5865\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 52228.1337 - accuracy: 0.5214\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2656.4944 - accuracy: 0.4405\n",
      "946/946 [==============================] - 29s 29ms/step - loss: 64421.9507 - accuracy: 0.5196\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 144.3254 - accuracy: 0.6421\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 53064.9745 - accuracy: 0.5201\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2805.5142 - accuracy: 0.6430\n",
      "946/946 [==============================] - 36s 36ms/step - loss: 60444.6687 - accuracy: 0.5139\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 3459.5676 - accuracy: 0.5609\n",
      "946/946 [==============================] - 33s 33ms/step - loss: 45628.4373 - accuracy: 0.5302\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3093.9719 - accuracy: 0.4401\n",
      "946/946 [==============================] - 33s 33ms/step - loss: 51742.5463 - accuracy: 0.5120\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3512.7732 - accuracy: 0.5628\n",
      "946/946 [==============================] - 31s 31ms/step - loss: 57083.9113 - accuracy: 0.5088\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 329.1201 - accuracy: 0.6527\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 47935.2818 - accuracy: 0.5102\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1516.8580 - accuracy: 0.6768\n",
      "946/946 [==============================] - 32s 33ms/step - loss: 61265.1919 - accuracy: 0.5307\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 581.6596 - accuracy: 0.6675\n",
      "946/946 [==============================] - 32s 32ms/step - loss: 65890.9551 - accuracy: 0.5187\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3980.8955 - accuracy: 0.6174\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 50687.0574 - accuracy: 0.5123\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 614.5514 - accuracy: 0.6648\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 46925.3374 - accuracy: 0.5116\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 10482.5439 - accuracy: 0.5626\n",
      "946/946 [==============================] - 31s 31ms/step - loss: 47072.7351 - accuracy: 0.5369\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2588.6826 - accuracy: 0.5558\n",
      "946/946 [==============================] - 31s 31ms/step - loss: 41009.8961 - accuracy: 0.5135\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 294.2713 - accuracy: 0.6679\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 50955.3645 - accuracy: 0.5323\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 1075.9625 - accuracy: 0.6703\n",
      "946/946 [==============================] - 31s 31ms/step - loss: 55383.0777 - accuracy: 0.5092\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 8542.8184 - accuracy: 0.5628\n",
      "946/946 [==============================] - 32s 32ms/step - loss: 53033.3492 - accuracy: 0.5334\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2338.8779 - accuracy: 0.5338\n",
      "946/946 [==============================] - 32s 33ms/step - loss: 51031.6779 - accuracy: 0.5129\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1673.6471 - accuracy: 0.6764\n",
      "946/946 [==============================] - 30s 31ms/step - loss: 61682.9049 - accuracy: 0.5322\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 13025.2568 - accuracy: 0.5821\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 59778.2857 - accuracy: 0.5294\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 334.2409 - accuracy: 0.6746\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 52062.2559 - accuracy: 0.5094\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3086.1482 - accuracy: 0.5963\n",
      "946/946 [==============================] - 34s 34ms/step - loss: 65909.1365 - accuracy: 0.5117\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 468.5335 - accuracy: 0.6650\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 40776.9871 - accuracy: 0.5195\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 1877.6975 - accuracy: 0.6413\n",
      "946/946 [==============================] - 33s 34ms/step - loss: 58450.4611 - accuracy: 0.5156\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 767.8636 - accuracy: 0.6734\n",
      "946/946 [==============================] - 34s 35ms/step - loss: 65417.3967 - accuracy: 0.5094\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 618.0659 - accuracy: 0.6416\n",
      "946/946 [==============================] - 32s 33ms/step - loss: 59900.0395 - accuracy: 0.5076\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 1557.5858 - accuracy: 0.6335\n",
      "946/946 [==============================] - 31s 31ms/step - loss: 46942.1764 - accuracy: 0.5275\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 5991.5293 - accuracy: 0.5626\n",
      "946/946 [==============================] - 33s 33ms/step - loss: 52216.0556 - accuracy: 0.5161\n",
      "237/237 [==============================] - 3s 9ms/step - loss: 1177.8818 - accuracy: 0.6294\n",
      "946/946 [==============================] - 34s 34ms/step - loss: 50459.8233 - accuracy: 0.5247\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 15826.1758 - accuracy: 0.5821\n",
      "946/946 [==============================] - 34s 35ms/step - loss: 78517.2101 - accuracy: 0.5105\n",
      "237/237 [==============================] - 3s 9ms/step - loss: 1756.1819 - accuracy: 0.6627\n",
      "946/946 [==============================] - 34s 35ms/step - loss: 58467.2447 - accuracy: 0.5310\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 14352.3115 - accuracy: 0.5628\n",
      "946/946 [==============================] - 32s 32ms/step - loss: 59463.5870 - accuracy: 0.5278\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1851.3190 - accuracy: 0.6658\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 56925.1109 - accuracy: 0.5015\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3822.4487 - accuracy: 0.6396\n",
      "946/946 [==============================] - 29s 30ms/step - loss: 66889.8417 - accuracy: 0.5163\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 3007.3406 - accuracy: 0.5821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946/946 [==============================] - 30s 31ms/step - loss: 51727.5420 - accuracy: 0.5087\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 948.0870 - accuracy: 0.6619\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 55743.3643 - accuracy: 0.5196\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1202.2084 - accuracy: 0.6940\n",
      "946/946 [==============================] - 33s 33ms/step - loss: 87016.2082 - accuracy: 0.5342\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 1368.7521 - accuracy: 0.5626\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 71311.6321 - accuracy: 0.5304\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 329.8559 - accuracy: 0.6311\n",
      "946/946 [==============================] - 32s 32ms/step - loss: 73207.1092 - accuracy: 0.5054\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 9360.8770 - accuracy: 0.5821\n",
      "946/946 [==============================] - 31s 31ms/step - loss: 49565.9838 - accuracy: 0.5238\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 1044.4874 - accuracy: 0.6725\n",
      "946/946 [==============================] - 30s 30ms/step - loss: 68089.9094 - accuracy: 0.5021\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 3645.5776 - accuracy: 0.6492\n",
      "473/473 [==============================] - 17s 33ms/step - loss: 47119.2396 - accuracy: 0.5209\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 1727.8302 - accuracy: 0.5495\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 59968.5920 - accuracy: 0.5146\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 586.6241 - accuracy: 0.6734\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 55792.9732 - accuracy: 0.5117\n",
      "119/119 [==============================] - 1s 10ms/step - loss: 841.2072 - accuracy: 0.6599\n",
      "473/473 [==============================] - 17s 35ms/step - loss: 84527.6337 - accuracy: 0.5069\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 10624.1328 - accuracy: 0.6115\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 70144.9060 - accuracy: 0.5102\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 4067.3738 - accuracy: 0.5628\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 61681.3454 - accuracy: 0.5239\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 5396.6353 - accuracy: 0.6671\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 64191.3825 - accuracy: 0.5047\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 2746.5645 - accuracy: 0.5385\n",
      "473/473 [==============================] - 20s 39ms/step - loss: 46680.4083 - accuracy: 0.5077\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 3976.0593 - accuracy: 0.5821\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 69552.6895 - accuracy: 0.5084\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 8039.3315 - accuracy: 0.5599\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 54719.9249 - accuracy: 0.5226\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 2933.3547 - accuracy: 0.5628\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 74258.5759 - accuracy: 0.5182\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 2015.2699 - accuracy: 0.5998\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 46525.3596 - accuracy: 0.5119\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 901.4016 - accuracy: 0.6794\n",
      "473/473 [==============================] - 17s 35ms/step - loss: 53913.6294 - accuracy: 0.5115\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 3102.1572 - accuracy: 0.6328\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 40344.1801 - accuracy: 0.5144\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 7192.3564 - accuracy: 0.5599\n",
      "473/473 [==============================] - 20s 39ms/step - loss: 62026.3891 - accuracy: 0.4955\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 12595.5527 - accuracy: 0.5848\n",
      "473/473 [==============================] - 17s 33ms/step - loss: 42597.6598 - accuracy: 0.5130\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 2891.5371 - accuracy: 0.5486\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 65548.0116 - accuracy: 0.5022\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 8764.6025 - accuracy: 0.5558\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 46013.2192 - accuracy: 0.5174\n",
      "119/119 [==============================] - 2s 10ms/step - loss: 7747.2344 - accuracy: 0.5821\n",
      "473/473 [==============================] - 19s 36ms/step - loss: 64544.5479 - accuracy: 0.4940\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 4819.3398 - accuracy: 0.6399\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 73183.2429 - accuracy: 0.5002\n",
      "119/119 [==============================] - 2s 10ms/step - loss: 8023.8140 - accuracy: 0.4372\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 55211.6777 - accuracy: 0.5159\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 1417.6029 - accuracy: 0.6400\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 66498.8884 - accuracy: 0.5116\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 4248.9688 - accuracy: 0.5952\n",
      "473/473 [==============================] - 17s 33ms/step - loss: 51854.7196 - accuracy: 0.50800s - loss: 527\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 3133.6106 - accuracy: 0.6544\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 52143.3302 - accuracy: 0.5164\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 1481.4832 - accuracy: 0.5599\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 53898.1781 - accuracy: 0.5142\n",
      "119/119 [==============================] - 2s 9ms/step - loss: 1932.3347 - accuracy: 0.6399\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 48728.3563 - accuracy: 0.5077\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 6292.1382 - accuracy: 0.5626\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 55411.9846 - accuracy: 0.5094\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 9244.3076 - accuracy: 0.5558\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 63335.6541 - accuracy: 0.4996\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 5817.1001 - accuracy: 0.5838\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 46996.5703 - accuracy: 0.5041\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 2004.8523 - accuracy: 0.5827\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 57655.2817 - accuracy: 0.5120\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 3766.0186 - accuracy: 0.5980\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 56713.9369 - accuracy: 0.5115\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 7136.9097 - accuracy: 0.4463\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 51595.7767 - accuracy: 0.4903\n",
      "119/119 [==============================] - 1s 10ms/step - loss: 1914.0009 - accuracy: 0.6396\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 59431.8537 - accuracy: 0.5246\n",
      "119/119 [==============================] - 1s 10ms/step - loss: 4999.6675 - accuracy: 0.5821\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 46390.6062 - accuracy: 0.5216\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 9275.9258 - accuracy: 0.5599\n",
      "473/473 [==============================] - 20s 39ms/step - loss: 53527.7928 - accuracy: 0.5207\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 3192.0820 - accuracy: 0.5628\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 42679.6329 - accuracy: 0.5083\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 1724.0979 - accuracy: 0.5220\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 68623.3733 - accuracy: 0.5045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 1s 8ms/step - loss: 2977.6621 - accuracy: 0.5698\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 46995.0721 - accuracy: 0.4988\n",
      "119/119 [==============================] - 2s 11ms/step - loss: 11646.6289 - accuracy: 0.5575\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 46836.6620 - accuracy: 0.5115\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 2760.5288 - accuracy: 0.6327\n",
      "473/473 [==============================] - 17s 33ms/step - loss: 42935.7147 - accuracy: 0.5093\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 1486.4537 - accuracy: 0.6796\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 52840.9766 - accuracy: 0.5223\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 1708.5081 - accuracy: 0.5626\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 69979.1817 - accuracy: 0.5124\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 62.6033 - accuracy: 0.6400\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 60109.6212 - accuracy: 0.5004\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 10659.7852 - accuracy: 0.5309\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 63207.8442 - accuracy: 0.5111\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 1621.0128 - accuracy: 0.5400\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 46960.3512 - accuracy: 0.5135\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 14467.2109 - accuracy: 0.5628\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 62523.3271 - accuracy: 0.5232\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 2897.0657 - accuracy: 0.5626\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 41552.9374 - accuracy: 0.5169\n",
      "119/119 [==============================] - 2s 10ms/step - loss: 8076.7549 - accuracy: 0.6624\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 51454.3984 - accuracy: 0.5089\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 5121.8574 - accuracy: 0.5821\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 65251.2988 - accuracy: 0.5264\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 6476.7285 - accuracy: 0.5518\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 45800.6502 - accuracy: 0.5148\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 176.5487 - accuracy: 0.6026\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 46502.7160 - accuracy: 0.5177\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 883.6297 - accuracy: 0.6282\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 64766.6191 - accuracy: 0.5162\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 8990.3760 - accuracy: 0.4442\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 54276.2441 - accuracy: 0.5176\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 1073.9181 - accuracy: 0.5821\n",
      "473/473 [==============================] - 18s 37ms/step - loss: 53759.2576 - accuracy: 0.5190\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 5538.2417 - accuracy: 0.5798\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 65837.2468 - accuracy: 0.5099\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 2922.7595 - accuracy: 0.6919\n",
      "473/473 [==============================] - 20s 39ms/step - loss: 61080.5615 - accuracy: 0.5084\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 1570.6123 - accuracy: 0.6688\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 46958.2170 - accuracy: 0.5187\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 8512.5156 - accuracy: 0.5558\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 76611.0878 - accuracy: 0.5130\n",
      "119/119 [==============================] - 2s 9ms/step - loss: 2512.8247 - accuracy: 0.5605\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 46647.7870 - accuracy: 0.5120\n",
      "119/119 [==============================] - 1s 10ms/step - loss: 6130.5811 - accuracy: 0.5159\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 46147.9822 - accuracy: 0.5142\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 3616.0190 - accuracy: 0.6153\n",
      "473/473 [==============================] - 17s 35ms/step - loss: 83562.2891 - accuracy: 0.5216\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 3447.9692 - accuracy: 0.6176\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 56482.6238 - accuracy: 0.5108\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 1523.9160 - accuracy: 0.6734\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 53689.6276 - accuracy: 0.5110\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 8746.4775 - accuracy: 0.5821\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 38823.1400 - accuracy: 0.5025\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 3546.4153 - accuracy: 0.5518\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 57227.9632 - accuracy: 0.5074\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 501.9959 - accuracy: 0.6928\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 65605.9541 - accuracy: 0.4980\n",
      "119/119 [==============================] - 1s 10ms/step - loss: 1638.2694 - accuracy: 0.6007\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 61818.1237 - accuracy: 0.5137\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 1065.2013 - accuracy: 0.5279\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 49597.1165 - accuracy: 0.5183\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 5826.5928 - accuracy: 0.5821\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 49107.7862 - accuracy: 0.5087\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 6572.9351 - accuracy: 0.5256\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 60155.9728 - accuracy: 0.5130\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 2040.7900 - accuracy: 0.5628\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 44200.0023 - accuracy: 0.5204\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 18156.4707 - accuracy: 0.4374\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 60502.2281 - accuracy: 0.5128\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 930.5234 - accuracy: 0.5943\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 45027.2426 - accuracy: 0.5113\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 616.7545 - accuracy: 0.5821\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 56164.0356 - accuracy: 0.5183\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 2364.2119 - accuracy: 0.6145\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 63610.2163 - accuracy: 0.5155\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 6018.0117 - accuracy: 0.5628\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 75456.8963 - accuracy: 0.5225\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 4008.8489 - accuracy: 0.5626\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 50482.9034 - accuracy: 0.5063\n",
      "119/119 [==============================] - 1s 10ms/step - loss: 1818.7754 - accuracy: 0.5558\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 48071.1946 - accuracy: 0.5235\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 11196.4150 - accuracy: 0.4932\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 61431.2265 - accuracy: 0.5009\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 1167.5018 - accuracy: 0.6124\n",
      "473/473 [==============================] - 19s 37ms/step - loss: 48822.3576 - accuracy: 0.5214\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 3960.0051 - accuracy: 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/473 [==============================] - 18s 35ms/step - loss: 43032.8343 - accuracy: 0.5107\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 432.4495 - accuracy: 0.5812\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 70278.7910 - accuracy: 0.5151\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 11013.5439 - accuracy: 0.5960\n",
      "473/473 [==============================] - 18s 37ms/step - loss: 69093.8227 - accuracy: 0.5120\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 3074.7195 - accuracy: 0.5821\n",
      "473/473 [==============================] - 18s 36ms/step - loss: 53700.8494 - accuracy: 0.5042\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 6504.0176 - accuracy: 0.4951\n",
      "473/473 [==============================] - 17s 33ms/step - loss: 48648.6272 - accuracy: 0.5130\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 10048.0176 - accuracy: 0.4372\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 74154.3684 - accuracy: 0.5058\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 1293.8049 - accuracy: 0.6354\n",
      "473/473 [==============================] - 17s 33ms/step - loss: 40997.2295 - accuracy: 0.5131\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 834.9147 - accuracy: 0.5558\n",
      "473/473 [==============================] - 18s 35ms/step - loss: 50183.1283 - accuracy: 0.5109\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 7494.6406 - accuracy: 0.5888\n",
      "473/473 [==============================] - 19s 38ms/step - loss: 62661.3158 - accuracy: 0.5198\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 15333.8438 - accuracy: 0.5599\n",
      "473/473 [==============================] - 17s 34ms/step - loss: 57226.8903 - accuracy: 0.5180\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 7838.4111 - accuracy: 0.5628\n",
      "1182/1182 [==============================] - 38s 31ms/step - loss: 48303.9474 - accuracy: 0.5104\n",
      "{'batch_size': 10, 'init': 'normal', 'nb_epoch': 150, 'optimizer': 'adam'}\n",
      "507/507 [==============================] - 4s 8ms/step - loss: 849.0076 - accuracy: 0.6061\n",
      "dnn: 0.6061204075813293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/violeta/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "/home/violeta/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2279: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/violeta/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2279: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.61%, Logloss: nan\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 62793.6673 - accuracy: 0.5185\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 2150.5308 - accuracy: 0.6074\n",
      "946/946 [==============================] - 32s 32ms/step - loss: 70411.5559 - accuracy: 0.5146\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 10382.1377 - accuracy: 0.5558\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 45497.5561 - accuracy: 0.5214\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 4525.9780 - accuracy: 0.4129\n",
      "946/946 [==============================] - 30s 31ms/step - loss: 47236.3204 - accuracy: 0.5247\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 4849.7622 - accuracy: 0.5599\n",
      "946/946 [==============================] - 31s 32ms/step - loss: 48315.4372 - accuracy: 0.5176\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 111.4891 - accuracy: 0.6657\n",
      "0.5603405714035035\n"
     ]
    }
   ],
   "source": [
    "DNN_binary = dnn_bin(x_train_binary_encoding,y_train_binary_encoding,x_validation_binary_encoding,y_validation_binary_encoding)\n",
    "test_model(DNN_binary,x_validation_binary_encoding,y_validation_binary_encoding)\n",
    "cross_val(DNN_binary, x_train_binary_encoding, y_train_binary_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x_train, y_train, x_validation, y_validation):\n",
    "    perceptron = Perceptron(tol=1e-3, random_state=0)\n",
    "    params_perc = {'alpha': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]}\n",
    "    per_gs = model_selection.GridSearchCV(perceptron, params_perc, cv=5)\n",
    "    per_gs.fit(x_train, y_train)\n",
    "    per_best = per_gs.best_estimator_\n",
    "    print(per_gs.best_params_)\n",
    "    print('perceptron: {}'.format(per_best.score(x_validation, y_validation)))\n",
    "    return per_gs\n",
    "\n",
    "def multi_perceptron(x_train, y_train, x_validation, y_validation):\n",
    "    mult_perceptron = MLPClassifier(tol=1e-3, random_state=0)\n",
    "    params_mult_perc =  {'hidden_layer_sizes': [(10,30,10),(20,)],'activation': ['tanh', 'relu'],'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "    mul_per_gs = model_selection.GridSearchCV(mult_perceptron, params_mult_perc, cv=5)\n",
    "    mul_per_gs.fit(x_train, y_train)\n",
    "    mul_per_best = mul_per_gs.best_estimator_\n",
    "    print(mul_per_gs.best_params_)\n",
    "    print('multi perceptron: {}'.format(mul_per_best.score(x_validation, y_validation)))\n",
    "    return mul_per_best\n",
    "\n",
    "def multi_perceptron_2(x_train, y_train, x_validation, y_validation):\n",
    "    mult_perceptron_2 = MLPRegressor()\n",
    "    params_mult_perc_2 = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (20,)],'activation': ['relu','tanh','logistic'],\n",
    "          'alpha': [0.0001, 0.05],'learning_rate': ['constant','adaptive'],\n",
    "          'solver': ['sgd', 'adam']}\n",
    "    mul_per_gs_2 = model_selection.GridSearchCV(mult_perceptron_2, params_mult_perc_2, cv=5)\n",
    "    mul_per_gs_2.fit(x_train, y_train)\n",
    "    mul_per_best_2 = mul_per_gs_2.best_estimator_\n",
    "    print(mul_per_gs_2.best_params_)\n",
    "    print('multi perceptron 2: {}'.format(mul_per_best_2.score(x_validation, y_validation)))\n",
    "    return mul_per_gs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "multi perceptron: 0.6639684106614018\n",
      "Accuracy: 66.40%, Logloss: 0.58\n",
      "0.6859104296139082\n"
     ]
    }
   ],
   "source": [
    "PERC_binary = multi_perceptron(x_train_binary_encoding,y_train_binary_encoding,x_validation_binary_encoding,y_validation_binary_encoding)\n",
    "test_model(PERC_binary,x_validation_binary_encoding,y_validation_binary_encoding)\n",
    "cross_val(PERC_binary, x_train_binary_encoding, y_train_binary_encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
