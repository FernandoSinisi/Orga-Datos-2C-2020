{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import WOEEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers import  Dropout, Dense, Activation\n",
    "from tensorflow.keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../Feature_Engineering/data/other-cleaned_train.csv')\n",
    "test = pd.read_csv('../../Feature_Engineering/data/other-cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "test.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.copy()\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_columns = train.drop(columns = [\"Opportunity_ID\",\"ID\", \"Pricing, Delivery_Terms_Quote_Appr\",\\\n",
    "                                    \"Bureaucratic_Code_0_Approval\",\"Bureaucratic_Code_0_Approved\",\\\n",
    "                                    \"Submitted_for_Approval\",\"ASP\",\"ASP_(converted)\",\"TRF\",\"Total_Amount\",\\\n",
    "                                    \"Total_Taxable_Amount\",\"diferencia_en_dias\",\"Last_Modified_DOY\",\"Last_Modified_Year\",\\\n",
    "                                    \"Opportunity_Created_DOY\",\"Opportunity_Created_Year\",\"Quote_Expiry_DOY\",\\\n",
    "                                     \"Quote_Expiry_Year\",\"Planned_Delivery_Start_DOY\",\"Planned_Delivery_Start_Year\",\\\n",
    "                                    \"Planned_Delivery_End_DOY\",\"Planned_Delivery_End_Year\",\\\n",
    "                                    \"Target\"]).columns\n",
    "for column in categ_columns:\n",
    "    encoder = WOEEncoder()\n",
    "    encoder.fit(train[column], train['Target'])\n",
    "    feature_encoded = encoder.transform(train[column])\n",
    "    X_train = X_train.join(feature_encoded.add_suffix('_woe'))\n",
    "    X_train.drop(columns=[column], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_columns = test.drop(columns = [\"Opportunity_ID\",\"ID\", \"Pricing, Delivery_Terms_Quote_Appr\",\\\n",
    "                                    \"Bureaucratic_Code_0_Approval\",\"Bureaucratic_Code_0_Approved\",\\\n",
    "                                    \"Submitted_for_Approval\",\"ASP\",\"ASP_(converted)\",\"TRF\",\"Total_Amount\",\\\n",
    "                                    \"Total_Taxable_Amount\",\"diferencia_en_dias\",\"Last_Modified_DOY\",\"Last_Modified_Year\",\\\n",
    "                                    \"Opportunity_Created_DOY\",\"Opportunity_Created_Year\",\"Quote_Expiry_DOY\",\\\n",
    "                                     \"Quote_Expiry_Year\",\"Planned_Delivery_Start_DOY\",\"Planned_Delivery_Start_Year\",\\\n",
    "                                    \"Planned_Delivery_End_DOY\",\"Planned_Delivery_End_Year\"]).columns\n",
    "for column in categ_columns:\n",
    "    encoder = WOEEncoder()\n",
    "    encoder.fit(train[column], train['Target'])\n",
    "    feature_encoded = encoder.transform(test[column])\n",
    "    X_test = X_test.join(feature_encoded.add_suffix('_woe'))\n",
    "    X_test.drop(columns=[column], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"Total_Amount\"] = pd.to_numeric(X_train[\"Total_Amount\"],errors='coerce').fillna(X_train[\"Total_Amount\"].mean())\n",
    "X_train[\"Opportunity_Created_Year\"] = pd.to_numeric(X_train[\"Opportunity_Created_Year\"],errors='coerce').fillna(0)\n",
    "X_train[\"Quote_Expiry_DOY\"] = pd.to_numeric(X_train[\"Quote_Expiry_DOY\"],errors='coerce').fillna(0)\n",
    "X_train[\"Quote_Expiry_Year\"] = pd.to_numeric(X_train[\"Quote_Expiry_Year\"],errors='coerce').fillna(0)\n",
    "X_train[\"Planned_Delivery_End_DOY\"] = pd.to_numeric(X_train[\"Planned_Delivery_End_DOY\"],errors='coerce').fillna(0)\n",
    "X_train[\"Planned_Delivery_End_Year\"] = pd.to_numeric(X_train[\"Planned_Delivery_End_Year\"],errors='coerce').fillna(0)\n",
    "\n",
    "X_train = X_train.drop(columns = 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"Total_Amount\"] = pd.to_numeric(X_test[\"Total_Amount\"],errors='coerce').fillna(test[\"Total_Amount\"].mean())\n",
    "X_test[\"Opportunity_Created_Year\"] = pd.to_numeric(X_test[\"Opportunity_Created_Year\"],errors='coerce').fillna(0)\n",
    "X_test[\"Quote_Expiry_DOY\"] = pd.to_numeric(X_test[\"Quote_Expiry_DOY\"],errors='coerce').fillna(0)\n",
    "X_test[\"Quote_Expiry_Year\"] = pd.to_numeric(X_test[\"Quote_Expiry_Year\"],errors='coerce').fillna(0)\n",
    "X_test[\"Planned_Delivery_End_DOY\"] = pd.to_numeric(X_test[\"Planned_Delivery_End_DOY\"],errors='coerce').fillna(0)\n",
    "X_test[\"Planned_Delivery_End_Year\"] = pd.to_numeric(X_test[\"Planned_Delivery_End_Year\"],errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model, x_train, y_train):\n",
    "    score_cross_val = model_selection.cross_val_score(model, x_train, y_train, cv=5)\n",
    "    print(score_cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_model(optimizer='rmsprop',init='glorot_uniform'):\n",
    "    node = 512\n",
    "    nClasses = 2\n",
    "    dropout=0.5\n",
    "    nFeatures = 55\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(node,input_dim=nFeatures,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,4):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(x_train, y_train, x_validation, y_validation):\n",
    "    keras_model = KerasClassifier(build_fn=DNN_model)\n",
    "    optimizers = ['rmsprop', 'adam']\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    epochs = [50, 100, 150]\n",
    "    batches = batches = [5, 10, 20]\n",
    "    param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init)\n",
    "    dnn_gs = GridSearchCV(keras_model, param_grid=param_grid)\n",
    "    dnn_gs.fit(x_train, y_train)\n",
    "    dnn_best = dnn_gs.best_estimator_\n",
    "    print(dnn_gs.best_params_)\n",
    "    print('dnn: {}'.format(dnn_best.score(x_validation, y_validation)))\n",
    "    return dnn_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x_test, y_test):\n",
    "    predictions = model.predict_proba(x_test)[:,1]\n",
    "    logloss = log_loss(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions.round())\n",
    "    print(\"Accuracy: %.2f%%, Logloss: %.2f\" % (accuracy*100.0, logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_features(model,train):\n",
    "    importance = model.feature_importances_\n",
    "    result = pd.DataFrame([train.columns,importance]).transpose()\n",
    "    result.columns = [\"Feature\",\"Importance\"]\n",
    "    return result.sort_values(by='Importance', ascending=False).head(15)[\"Feature\"].to_list()\n",
    "    \n",
    "    \n",
    "def plot_features(model,train):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(350, 350)\n",
    "    selection = SelectFromModel(model, threshold=0.040, prefit=True)\n",
    "    selected_dataset = selection.transform(train)\n",
    "    model.plot_importance(booster=model)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (40,20)\n",
    "    plt.xlabel(\"\\nFeature importance\", fontsize=40)\n",
    "    plt.ylabel(\"Features\", fontsize=35)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.Target\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(X_train, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891/1891 [==============================] - 5s 2ms/step - loss: 50454.0372 - accuracy: 0.5096\n",
      "473/473 [==============================] - 0s 695us/step - loss: 231.3257 - accuracy: 0.6717\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 56234.5441 - accuracy: 0.5206\n",
      "473/473 [==============================] - 1s 843us/step - loss: 1625.1417 - accuracy: 0.5745\n",
      "1891/1891 [==============================] - 5s 3ms/step - loss: 63413.1109 - accuracy: 0.5170\n",
      "473/473 [==============================] - 1s 884us/step - loss: 1264.1934 - accuracy: 0.5580\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 70095.3474 - accuracy: 0.5217\n",
      "473/473 [==============================] - 1s 855us/step - loss: 881.1698 - accuracy: 0.6644\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 94043.4523 - accuracy: 0.5162\n",
      "473/473 [==============================] - 1s 835us/step - loss: 1225.7369 - accuracy: 0.5540\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 65789.4328 - accuracy: 0.5074\n",
      "473/473 [==============================] - 0s 746us/step - loss: 1145.4497 - accuracy: 0.6806\n",
      "1891/1891 [==============================] - 4s 2ms/step - loss: 92955.5374 - accuracy: 0.5094\n",
      "473/473 [==============================] - 1s 837us/step - loss: 3076.3726 - accuracy: 0.4594\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 72876.5689 - accuracy: 0.5076\n",
      "473/473 [==============================] - 0s 802us/step - loss: 89.2980 - accuracy: 0.6489\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 114041.3282 - accuracy: 0.5173\n",
      "473/473 [==============================] - 0s 791us/step - loss: 5456.7202 - accuracy: 0.4325\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 64418.7424 - accuracy: 0.5015\n",
      "473/473 [==============================] - 1s 940us/step - loss: 718.2692 - accuracy: 0.5493\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 62666.4343 - accuracy: 0.5229\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 783.5560 - accuracy: 0.6447\n",
      "1891/1891 [==============================] - 5s 3ms/step - loss: 63933.6312 - accuracy: 0.5379\n",
      "473/473 [==============================] - 0s 791us/step - loss: 318.0633 - accuracy: 0.5444\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 49725.8502 - accuracy: 0.5280\n",
      "473/473 [==============================] - 0s 816us/step - loss: 3034.1643 - accuracy: 0.6108\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 82818.7972 - accuracy: 0.5217\n",
      "473/473 [==============================] - 1s 913us/step - loss: 1585.6870 - accuracy: 0.5671\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 63061.6552 - accuracy: 0.5240\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1544.7078 - accuracy: 0.5540\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 53611.4432 - accuracy: 0.5202\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 375.6845 - accuracy: 0.5508\n",
      "1891/1891 [==============================] - 8s 4ms/step - loss: 57901.6502 - accuracy: 0.5206\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 911.6959 - accuracy: 0.5723\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 62409.7288 - accuracy: 0.5216\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1423.9292 - accuracy: 0.6417\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 54147.3070 - accuracy: 0.5196\n",
      "473/473 [==============================] - 1s 978us/step - loss: 144.4507 - accuracy: 0.5671\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 97736.0393 - accuracy: 0.5226\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 4608.6538 - accuracy: 0.6547\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 45961.8649 - accuracy: 0.5069\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 282.5992 - accuracy: 0.5508\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 59862.9226 - accuracy: 0.5138\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1482.5403 - accuracy: 0.6227\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 78997.3042 - accuracy: 0.5199\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 319.1126 - accuracy: 0.6713\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 71209.2853 - accuracy: 0.5221\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 3454.5027 - accuracy: 0.4431\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 45506.2988 - accuracy: 0.5179\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2509.1423 - accuracy: 0.4879\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 44821.8704 - accuracy: 0.5243\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 5107.8984 - accuracy: 0.4996\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 120020.5986 - accuracy: 0.5331\n",
      "473/473 [==============================] - 1s 901us/step - loss: 393.4456 - accuracy: 0.6586\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 69407.3719 - accuracy: 0.5292\n",
      "473/473 [==============================] - 1s 999us/step - loss: 1684.9860 - accuracy: 0.6772\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 70813.0854 - accuracy: 0.5172\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 133.5611 - accuracy: 0.6733\n",
      "1891/1891 [==============================] - 5s 3ms/step - loss: 73133.6969 - accuracy: 0.5164\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 313.8745 - accuracy: 0.6411\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 79243.3116 - accuracy: 0.5233\n",
      "473/473 [==============================] - 1s 948us/step - loss: 420.1573 - accuracy: 0.6290\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 96071.7246 - accuracy: 0.5280\n",
      "473/473 [==============================] - 1s 993us/step - loss: 3003.1116 - accuracy: 0.5745\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 96396.0102 - accuracy: 0.5335\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2683.8254 - accuracy: 0.5372\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 59536.3040 - accuracy: 0.5072\n",
      "473/473 [==============================] - 1s 974us/step - loss: 2610.9592 - accuracy: 0.5920\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 88587.8259 - accuracy: 0.5337\n",
      "473/473 [==============================] - 1s 915us/step - loss: 1576.1877 - accuracy: 0.6737\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 43701.2665 - accuracy: 0.5150\n",
      "473/473 [==============================] - 1s 951us/step - loss: 360.1700 - accuracy: 0.5127\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 86545.7554 - accuracy: 0.5268\n",
      "473/473 [==============================] - 1s 980us/step - loss: 932.7418 - accuracy: 0.4552\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 71738.0775 - accuracy: 0.5161\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 260.7889 - accuracy: 0.6802\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 49953.1797 - accuracy: 0.4971\n",
      "473/473 [==============================] - 1s 942us/step - loss: 905.9924 - accuracy: 0.6267\n",
      "1891/1891 [==============================] - 5s 2ms/step - loss: 82144.1480 - accuracy: 0.5164\n",
      "473/473 [==============================] - 1s 988us/step - loss: 5440.2554 - accuracy: 0.6382\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 100520.4202 - accuracy: 0.5142\n",
      "473/473 [==============================] - 1s 944us/step - loss: 1920.7102 - accuracy: 0.6324\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 96255.4003 - accuracy: 0.5198\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1438.8114 - accuracy: 0.6341\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 42928.3465 - accuracy: 0.5076\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2783.9089 - accuracy: 0.5728\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 66434.7950 - accuracy: 0.5069\n",
      "473/473 [==============================] - 1s 981us/step - loss: 8504.0918 - accuracy: 0.5620\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 62703.6386 - accuracy: 0.5119\n",
      "473/473 [==============================] - 1s 957us/step - loss: 4828.3296 - accuracy: 0.5540\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 41772.9447 - accuracy: 0.5116\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2229.1677 - accuracy: 0.5508\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 70945.0651 - accuracy: 0.5207\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 11978.6846 - accuracy: 0.5740\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 71210.7893 - accuracy: 0.5106\n",
      "473/473 [==============================] - 1s 980us/step - loss: 1329.3944 - accuracy: 0.6705\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 82673.1906 - accuracy: 0.5316\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 4937.3354 - accuracy: 0.5671\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 125674.4224 - accuracy: 0.5206\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 796.2789 - accuracy: 0.5540\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 36409.9601 - accuracy: 0.5198\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 530.9921 - accuracy: 0.5245\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 68424.1983 - accuracy: 0.5184\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1610.9587 - accuracy: 0.5410\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 55361.3569 - accuracy: 0.5245\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 3839.5342 - accuracy: 0.5770\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 65309.9391 - accuracy: 0.5140\n",
      "473/473 [==============================] - 1s 984us/step - loss: 3913.2014 - accuracy: 0.6111\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 104776.3147 - accuracy: 0.5259\n",
      "473/473 [==============================] - 1s 999us/step - loss: 13827.9268 - accuracy: 0.5540\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 60451.2314 - accuracy: 0.5195\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2839.2129 - accuracy: 0.5508\n",
      "1891/1891 [==============================] - 7s 4ms/step - loss: 62200.5948 - accuracy: 0.5278\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 6569.3237 - accuracy: 0.4683\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 54995.3820 - accuracy: 0.5196\n",
      "473/473 [==============================] - 1s 985us/step - loss: 709.9510 - accuracy: 0.5964\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 59767.6056 - accuracy: 0.5168\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1042.0565 - accuracy: 0.6750\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 59236.4088 - accuracy: 0.5164\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1656.1456 - accuracy: 0.6526\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 80106.3801 - accuracy: 0.5194\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 7656.4780 - accuracy: 0.5508\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 46776.0056 - accuracy: 0.5211\n",
      "473/473 [==============================] - 1s 992us/step - loss: 268.8279 - accuracy: 0.6510\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 54080.3985 - accuracy: 0.5412\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2050.3933 - accuracy: 0.5770\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 62402.6185 - accuracy: 0.5214\n",
      "473/473 [==============================] - 1s 974us/step - loss: 4287.1104 - accuracy: 0.5671\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 42228.2093 - accuracy: 0.5156\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1659.5056 - accuracy: 0.5569\n",
      "1891/1891 [==============================] - 8s 4ms/step - loss: 109613.7910 - accuracy: 0.5080\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 781.6066 - accuracy: 0.6616\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 74379.4649 - accuracy: 0.5263\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 774.7503 - accuracy: 0.5063\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 64214.7261 - accuracy: 0.5300\n",
      "473/473 [==============================] - 1s 973us/step - loss: 2992.6384 - accuracy: 0.6616\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 78260.8599 - accuracy: 0.5242\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2541.8892 - accuracy: 0.5074\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 65000.6652 - accuracy: 0.5245\n",
      "473/473 [==============================] - 1s 983us/step - loss: 1810.2495 - accuracy: 0.6039\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 109786.9788 - accuracy: 0.5106\n",
      "473/473 [==============================] - 1s 968us/step - loss: 916.9564 - accuracy: 0.6544\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 57489.6012 - accuracy: 0.5416\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 784.7507 - accuracy: 0.6578\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 46941.8123 - accuracy: 0.5179\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2576.5991 - accuracy: 0.4607\n",
      "1891/1891 [==============================] - 8s 4ms/step - loss: 76633.2685 - accuracy: 0.5237\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 4381.8652 - accuracy: 0.6196\n",
      "1891/1891 [==============================] - 8s 4ms/step - loss: 107599.3308 - accuracy: 0.5160\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2107.4119 - accuracy: 0.5540\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 53215.5995 - accuracy: 0.5193\n",
      "473/473 [==============================] - 1s 984us/step - loss: 3554.4370 - accuracy: 0.5563\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 58291.1026 - accuracy: 0.5200\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 363.9728 - accuracy: 0.6561\n",
      "1891/1891 [==============================] - 6s 3ms/step - loss: 68830.4129 - accuracy: 0.5124\n",
      "473/473 [==============================] - 1s 972us/step - loss: 1298.7588 - accuracy: 0.6523\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 51734.0848 - accuracy: 0.5307\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 301.0987 - accuracy: 0.6775\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 73752.9337 - accuracy: 0.5027\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 3470.7466 - accuracy: 0.5540\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 59382.2968 - accuracy: 0.5211\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1246.7048 - accuracy: 0.6832\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 75262.9506 - accuracy: 0.5153\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2135.8916 - accuracy: 0.5190\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 58147.8187 - accuracy: 0.5273\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 464.7374 - accuracy: 0.6134\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 67486.3754 - accuracy: 0.5125\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 583.4297 - accuracy: 0.5671\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 53584.8587 - accuracy: 0.5090\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2478.8611 - accuracy: 0.6297\n",
      "1891/1891 [==============================] - 8s 4ms/step - loss: 53591.4168 - accuracy: 0.5195\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 3673.4919 - accuracy: 0.4480\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 54554.5834 - accuracy: 0.5159\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 3059.7039 - accuracy: 0.6019\n",
      "1891/1891 [==============================] - 7s 3ms/step - loss: 65439.7084 - accuracy: 0.5322\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 3865.8677 - accuracy: 0.4662\n",
      "1891/1891 [==============================] - 7s 4ms/step - loss: 58428.7002 - accuracy: 0.5184\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 2726.0232 - accuracy: 0.5916\n",
      "1891/1891 [==============================] - 7s 4ms/step - loss: 118785.8435 - accuracy: 0.5081\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 5052.2256 - accuracy: 0.5540\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 61706.1904 - accuracy: 0.5099\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 860.0355 - accuracy: 0.6607\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 76633.8171 - accuracy: 0.5098\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2229.2659 - accuracy: 0.6527\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 48653.5735 - accuracy: 0.5156\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2007.0465 - accuracy: 0.5770\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 56191.9133 - accuracy: 0.5236\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1419.2994 - accuracy: 0.6327\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 77271.1609 - accuracy: 0.5186\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 6639.9253 - accuracy: 0.4803\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 48467.7422 - accuracy: 0.5009\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1720.1260 - accuracy: 0.6705\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 52024.9694 - accuracy: 0.5105\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 555.6299 - accuracy: 0.6705\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 64019.2913 - accuracy: 0.5275\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 407.7792 - accuracy: 0.6789\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 71256.3154 - accuracy: 0.5180\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 8663.0029 - accuracy: 0.5671\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 67016.9719 - accuracy: 0.5159\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 911.1592 - accuracy: 0.5540\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 53182.1119 - accuracy: 0.5043\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2169.3264 - accuracy: 0.5508\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 88159.9585 - accuracy: 0.5055\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1028.9493 - accuracy: 0.6079\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 49274.6219 - accuracy: 0.5171\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1890.5436 - accuracy: 0.6104\n",
      "946/946 [==============================] - 4s 3ms/step - loss: 48128.0753 - accuracy: 0.5185\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 3419.8057 - accuracy: 0.6162\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 75631.2009 - accuracy: 0.5137\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 306.2826 - accuracy: 0.6416\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 39858.6075 - accuracy: 0.5208\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 496.6901 - accuracy: 0.6324\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 63310.4570 - accuracy: 0.5187\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 887.0983 - accuracy: 0.5745\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 53800.3384 - accuracy: 0.5148\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1295.0631 - accuracy: 0.6015\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 54933.4898 - accuracy: 0.5205\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 7793.2808 - accuracy: 0.5700\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 57369.7587 - accuracy: 0.4865\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 971.8113 - accuracy: 0.5540\n",
      "946/946 [==============================] - 4s 3ms/step - loss: 63763.9055 - accuracy: 0.5201\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2309.8809 - accuracy: 0.5000\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 78123.2374 - accuracy: 0.5091\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1121.5341 - accuracy: 0.5004\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 58424.7600 - accuracy: 0.5265\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 74.7294 - accuracy: 0.6307\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 61920.6330 - accuracy: 0.5157\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 697.0137 - accuracy: 0.6746\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 73742.5751 - accuracy: 0.5305\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2384.7393 - accuracy: 0.5540\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 54326.8757 - accuracy: 0.5230\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 129.6650 - accuracy: 0.6789\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 63588.9801 - accuracy: 0.5174\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2239.0078 - accuracy: 0.6426\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 66349.7930 - accuracy: 0.5212\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 462.5796 - accuracy: 0.4543\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 84924.0048 - accuracy: 0.5170\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 9548.0186 - accuracy: 0.5671\n",
      "946/946 [==============================] - 4s 3ms/step - loss: 60498.7781 - accuracy: 0.5100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1176.3108 - accuracy: 0.5540\n",
      "946/946 [==============================] - 4s 3ms/step - loss: 67984.2735 - accuracy: 0.5001\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 579.1343 - accuracy: 0.6383\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 53962.2663 - accuracy: 0.5114\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1059.5812 - accuracy: 0.5964\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 47146.3904 - accuracy: 0.5128\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 13356.4609 - accuracy: 0.5770\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 63718.1172 - accuracy: 0.5189\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 7506.4199 - accuracy: 0.4329\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 105211.3268 - accuracy: 0.5183\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1233.0596 - accuracy: 0.6162\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 54831.8190 - accuracy: 0.5048\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1011.0974 - accuracy: 0.5508\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 46620.8613 - accuracy: 0.5139\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 4008.0005 - accuracy: 0.5470\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 75133.2108 - accuracy: 0.5054\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1397.6744 - accuracy: 0.6024\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 75973.8800 - accuracy: 0.5236\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 460.5344 - accuracy: 0.5671\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 54961.5023 - accuracy: 0.4998\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 124.8807 - accuracy: 0.5540\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 50851.0099 - accuracy: 0.5196\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 314.0454 - accuracy: 0.6667\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 56353.2116 - accuracy: 0.5141\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1074.6029 - accuracy: 0.6303\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 62629.8741 - accuracy: 0.5110\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1429.7257 - accuracy: 0.5808\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 61985.8362 - accuracy: 0.5200\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2725.7920 - accuracy: 0.6030\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 60874.1075 - accuracy: 0.5202\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1233.8607 - accuracy: 0.5540\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 44625.1063 - accuracy: 0.5097\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 5568.0352 - accuracy: 0.5508\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 46459.9849 - accuracy: 0.5319\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 934.4092 - accuracy: 0.6557\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 67053.6597 - accuracy: 0.5192\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 3786.2432 - accuracy: 0.6210\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 63100.8578 - accuracy: 0.5389\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 26729.3848 - accuracy: 0.4333\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 67295.2157 - accuracy: 0.5267\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1304.2101 - accuracy: 0.5032\n",
      "946/946 [==============================] - 4s 3ms/step - loss: 49172.5797 - accuracy: 0.5182\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1302.6460 - accuracy: 0.6781\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 68360.6864 - accuracy: 0.5237\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2492.1606 - accuracy: 0.5943\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 48087.9984 - accuracy: 0.5147\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 403.0598 - accuracy: 0.5668\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 58925.9489 - accuracy: 0.5027\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 426.6770 - accuracy: 0.5671\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 59861.3267 - accuracy: 0.5103\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2136.1997 - accuracy: 0.5540\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 62185.9714 - accuracy: 0.5079\n",
      "237/237 [==============================] - 1s 1ms/step - loss: 3432.7080 - accuracy: 0.5508\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 49628.6976 - accuracy: 0.5198\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 499.8438 - accuracy: 0.5664\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 49949.0431 - accuracy: 0.5136\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1857.7749 - accuracy: 0.6015\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 70750.0938 - accuracy: 0.5175\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 769.0839 - accuracy: 0.6729\n",
      "946/946 [==============================] - 6s 6ms/step - loss: 64097.3157 - accuracy: 0.5275\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 12089.8662 - accuracy: 0.5540\n",
      "946/946 [==============================] - 6s 5ms/step - loss: 58936.4303 - accuracy: 0.5137\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 365.8539 - accuracy: 0.5508\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 52072.7521 - accuracy: 0.5233\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 361.1144 - accuracy: 0.6730\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 56068.7074 - accuracy: 0.5143\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2255.9810 - accuracy: 0.5770\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 61495.7604 - accuracy: 0.5108\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1953.8754 - accuracy: 0.4397\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 72544.1481 - accuracy: 0.4991\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2876.2468 - accuracy: 0.6162\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 61362.6247 - accuracy: 0.4921\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 7362.0684 - accuracy: 0.5161\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 69844.9277 - accuracy: 0.5387\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1263.2462 - accuracy: 0.6383\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 50749.3334 - accuracy: 0.5168\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 9305.8105 - accuracy: 0.5770\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 48900.4356 - accuracy: 0.5029\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 576.9047 - accuracy: 0.5654\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 124936.9910 - accuracy: 0.5162\n",
      "237/237 [==============================] - 1s 1ms/step - loss: 1648.5790 - accuracy: 0.5425\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 52076.6063 - accuracy: 0.5048\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 882.0148 - accuracy: 0.4907\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 80888.3024 - accuracy: 0.5186\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1106.1217 - accuracy: 0.5981\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 60483.6502 - accuracy: 0.5074\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 4453.9492 - accuracy: 0.6557\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 55259.1013 - accuracy: 0.5091\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 11343.9570 - accuracy: 0.4329\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 89261.3110 - accuracy: 0.5147\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2212.8748 - accuracy: 0.6356\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 36473.9510 - accuracy: 0.5078\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 4016.4231 - accuracy: 0.5508\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 57380.3462 - accuracy: 0.5107\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 3851.4312 - accuracy: 0.5745\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 37771.6938 - accuracy: 0.5102\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2938.9360 - accuracy: 0.6138\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 54743.7008 - accuracy: 0.5161\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2217.0564 - accuracy: 0.5612\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 44761.2824 - accuracy: 0.5182\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 863.2697 - accuracy: 0.6716\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 63371.8732 - accuracy: 0.5162\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1510.6927 - accuracy: 0.5508\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 56681.2402 - accuracy: 0.5267\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 338.0734 - accuracy: 0.6252\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 50780.1626 - accuracy: 0.5199\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 514.5798 - accuracy: 0.5791\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 52826.1826 - accuracy: 0.5219\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1426.5736 - accuracy: 0.6754\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 52605.0573 - accuracy: 0.5141\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 3654.1418 - accuracy: 0.5413\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 57183.1336 - accuracy: 0.5113\n",
      "237/237 [==============================] - 1s 1ms/step - loss: 366.3788 - accuracy: 0.6459\n",
      "946/946 [==============================] - 4s 4ms/step - loss: 58549.3702 - accuracy: 0.5327\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 1599.5339 - accuracy: 0.5389\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 60112.3744 - accuracy: 0.5147\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 272.7762 - accuracy: 0.6582\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 67024.3453 - accuracy: 0.5230\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 71.5473 - accuracy: 0.6238\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 54764.3119 - accuracy: 0.5165\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2890.0918 - accuracy: 0.4744\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 44888.5819 - accuracy: 0.5020\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3264.8411 - accuracy: 0.5618\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48450.8394 - accuracy: 0.5260\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1868.1144 - accuracy: 0.6223\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 64086.5521 - accuracy: 0.5009\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 4078.9978 - accuracy: 0.6781\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 49547.2344 - accuracy: 0.5072\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3411.7209 - accuracy: 0.6217\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 49124.4794 - accuracy: 0.5025\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 369.7476 - accuracy: 0.6589\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 62839.6378 - accuracy: 0.5218\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 807.4013 - accuracy: 0.6713\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 52562.8159 - accuracy: 0.5085\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 6436.0913 - accuracy: 0.5745\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 68546.7651 - accuracy: 0.5279\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 5406.5732 - accuracy: 0.5609\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 58603.4571 - accuracy: 0.5150\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 4526.0615 - accuracy: 0.5671\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 59961.3816 - accuracy: 0.5104\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3299.8674 - accuracy: 0.5540\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 106175.9464 - accuracy: 0.5123\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1354.9934 - accuracy: 0.6633\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 46192.2348 - accuracy: 0.5106\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 893.6173 - accuracy: 0.5279\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48831.3840 - accuracy: 0.5009\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2588.5449 - accuracy: 0.6151\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48815.2489 - accuracy: 0.4973\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1015.2862 - accuracy: 0.6703\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 69638.5483 - accuracy: 0.5155\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2818.7168 - accuracy: 0.6746\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 91542.2540 - accuracy: 0.5037\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1212.2657 - accuracy: 0.6041\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 68981.9602 - accuracy: 0.5273\n",
      "119/119 [==============================] - 1s 4ms/step - loss: 2313.4138 - accuracy: 0.5867\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 46840.4819 - accuracy: 0.5019\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1080.9683 - accuracy: 0.6316\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 54455.7926 - accuracy: 0.5166\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1220.5647 - accuracy: 0.5772\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 50900.1104 - accuracy: 0.5081\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 9693.7939 - accuracy: 0.5540\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 57403.5612 - accuracy: 0.5097\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 5911.9937 - accuracy: 0.5508\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 77087.2452 - accuracy: 0.4989\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1645.8999 - accuracy: 0.6049\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 54926.7853 - accuracy: 0.4974\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 3244.5930 - accuracy: 0.5770\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 52000.4135 - accuracy: 0.5116\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2597.2437 - accuracy: 0.6255\n",
      "473/473 [==============================] - 4s 6ms/step - loss: 74923.7471 - accuracy: 0.5122\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2560.0776 - accuracy: 0.6085\n",
      "473/473 [==============================] - 3s 6ms/step - loss: 41682.5349 - accuracy: 0.5097\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 657.5311 - accuracy: 0.5199\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 66367.5637 - accuracy: 0.4976\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1266.0706 - accuracy: 0.6172\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 49073.6469 - accuracy: 0.5118\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2254.0513 - accuracy: 0.6138\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48023.4618 - accuracy: 0.5164\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 6316.9980 - accuracy: 0.5734\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 63775.8729 - accuracy: 0.4943\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 173.6846 - accuracy: 0.6623\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 51051.1759 - accuracy: 0.5277\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1114.7565 - accuracy: 0.6696\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48745.0114 - accuracy: 0.5092\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2002.4330 - accuracy: 0.6294\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 50661.4725 - accuracy: 0.5039\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 27568.2715 - accuracy: 0.5770\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 69694.8894 - accuracy: 0.5026\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3544.0183 - accuracy: 0.5671\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 60314.5875 - accuracy: 0.4987\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 12398.4014 - accuracy: 0.4460\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 60115.0247 - accuracy: 0.5198\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 21287.0059 - accuracy: 0.5508\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 75126.2547 - accuracy: 0.5149\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1239.1049 - accuracy: 0.5389\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 46298.8776 - accuracy: 0.5168\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2039.9325 - accuracy: 0.6586\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 73572.7059 - accuracy: 0.5125\n",
      "119/119 [==============================] - 1s 4ms/step - loss: 3365.2393 - accuracy: 0.5861\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 65362.0867 - accuracy: 0.5094\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1790.9164 - accuracy: 0.5772\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 60206.5563 - accuracy: 0.5072\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 7028.6812 - accuracy: 0.5618\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48346.7407 - accuracy: 0.5149\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1943.9275 - accuracy: 0.5385\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 62357.0498 - accuracy: 0.5028\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 11473.0537 - accuracy: 0.5770\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 60379.1129 - accuracy: 0.5078\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2024.1788 - accuracy: 0.6729\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 70962.7656 - accuracy: 0.5303\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 4449.2554 - accuracy: 0.5540\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 52380.6779 - accuracy: 0.5149\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1686.9921 - accuracy: 0.6591\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 69395.8627 - accuracy: 0.5298\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1710.3065 - accuracy: 0.6214\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 54612.0188 - accuracy: 0.5009\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 4888.2681 - accuracy: 0.5854\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 64705.6838 - accuracy: 0.5009\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 493.5110 - accuracy: 0.6708\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 75974.4222 - accuracy: 0.5033\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 4036.2517 - accuracy: 0.5832\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 58019.8878 - accuracy: 0.5071\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 5759.8081 - accuracy: 0.5508\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 58413.0387 - accuracy: 0.4951\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3343.2722 - accuracy: 0.5740\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 70908.0114 - accuracy: 0.5138\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1294.9635 - accuracy: 0.5931\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 60357.3161 - accuracy: 0.5065\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1450.4242 - accuracy: 0.5671\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 66369.4823 - accuracy: 0.5112\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 10078.3779 - accuracy: 0.5540\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 44450.0350 - accuracy: 0.5081\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 4661.5430 - accuracy: 0.4886\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 61560.4146 - accuracy: 0.5173\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 4412.6025 - accuracy: 0.4412\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 55186.7379 - accuracy: 0.5172\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 12411.8359 - accuracy: 0.5770\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 71592.8010 - accuracy: 0.5185\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 14210.0498 - accuracy: 0.5709\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 75543.6936 - accuracy: 0.5276\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1196.7346 - accuracy: 0.6767\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48948.5384 - accuracy: 0.5247\n",
      "119/119 [==============================] - 1s 4ms/step - loss: 1203.9022 - accuracy: 0.6764\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 45289.8791 - accuracy: 0.5076\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 204.1505 - accuracy: 0.5918\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 69828.0561 - accuracy: 0.5174\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2299.2905 - accuracy: 0.6768\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 83082.5361 - accuracy: 0.5118\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 8811.1523 - accuracy: 0.5798\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 50959.6369 - accuracy: 0.5068\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2088.0186 - accuracy: 0.6005\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 56146.1797 - accuracy: 0.4961\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 362.8819 - accuracy: 0.6603\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 50431.1360 - accuracy: 0.5058\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2210.0415 - accuracy: 0.5470\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 54487.9258 - accuracy: 0.5158\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 8526.3369 - accuracy: 0.5305\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 64881.1669 - accuracy: 0.5052\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 4683.4331 - accuracy: 0.6162\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 48096.0696 - accuracy: 0.5052\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 926.6360 - accuracy: 0.6758\n",
      "473/473 [==============================] - 2s 4ms/step - loss: 53689.8193 - accuracy: 0.5092\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1527.9275 - accuracy: 0.6747\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 68575.9401 - accuracy: 0.5031\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 8253.1875 - accuracy: 0.5745\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 59582.1889 - accuracy: 0.5164\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 5801.9917 - accuracy: 0.5770\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 36017.5149 - accuracy: 0.5108\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2033.6289 - accuracy: 0.6568\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 52653.2986 - accuracy: 0.5026\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 3833.6929 - accuracy: 0.6157\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 52574.9283 - accuracy: 0.5175\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1716.4587 - accuracy: 0.6117\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 52209.6559 - accuracy: 0.5172\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 11689.4053 - accuracy: 0.5745\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 45170.9735 - accuracy: 0.5160\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 6442.7466 - accuracy: 0.5317\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 74693.3350 - accuracy: 0.5116\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 5971.3257 - accuracy: 0.5806\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 70275.9072 - accuracy: 0.4968\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 15277.8701 - accuracy: 0.5540\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 48715.0771 - accuracy: 0.4929\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3208.0042 - accuracy: 0.6062\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 57385.1733 - accuracy: 0.5076\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 423.4859 - accuracy: 0.6286\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 40985.0202 - accuracy: 0.5101\n",
      "119/119 [==============================] - 1s 4ms/step - loss: 25141.3633 - accuracy: 0.5770\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 76707.1788 - accuracy: 0.5076\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 523.3196 - accuracy: 0.6771\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 43491.1988 - accuracy: 0.5148\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2729.4565 - accuracy: 0.5925\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 54263.7481 - accuracy: 0.5052\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2054.4663 - accuracy: 0.5647\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 64280.1296 - accuracy: 0.5150\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 923.0461 - accuracy: 0.6311\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 50057.1264 - accuracy: 0.5198\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 1953.5967 - accuracy: 0.6785\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 64840.5957 - accuracy: 0.5239\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 4340.3057 - accuracy: 0.6272\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 65739.9401 - accuracy: 0.5028\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 14247.8477 - accuracy: 0.5540\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 54849.9263 - accuracy: 0.5179\n",
      "{'batch_size': 20, 'init': 'glorot_uniform', 'nb_epoch': 100, 'optimizer': 'rmsprop'}\n",
      "254/254 [==============================] - 1s 1ms/step - loss: 14606.8438 - accuracy: 0.5647\n",
      "dnn: 0.5646594166755676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.47%, Logloss: nan\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 50825.2810 - accuracy: 0.5076\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2466.0745 - accuracy: 0.5998\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 53573.1530 - accuracy: 0.5175\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3538.7458 - accuracy: 0.5745\n",
      "473/473 [==============================] - 2s 4ms/step - loss: 54348.5918 - accuracy: 0.5087\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 3129.1558 - accuracy: 0.6768\n",
      "473/473 [==============================] - 3s 4ms/step - loss: 60682.2655 - accuracy: 0.5079\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2210.4236 - accuracy: 0.5658\n",
      "473/473 [==============================] - 3s 5ms/step - loss: 61811.2370 - accuracy: 0.5087\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 2420.1030 - accuracy: 0.6369\n",
      "0.6107616543769836\n"
     ]
    }
   ],
   "source": [
    "dnn_model = dnn(x_train, y_train, x_validation, y_validation)\n",
    "test_model(dnn_model,x_validation,y_validation)\n",
    "cross_val(dnn_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasClassifier' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4c2ddda5cc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_featu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Opportunity_ID\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_featu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbest_featu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Opportunity_ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-824fd1eb28c8>\u001b[0m in \u001b[0;36mbest_features\u001b[0;34m(model, train)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbest_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimportance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimportance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Feature\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Importance\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Importance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Feature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "best_featu = best_features(dnn_model,X_train)\n",
    "if \"Opportunity_ID\" not in best_featu: \n",
    "    best_featu.append(\"Opportunity_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "y_pred = dnn_model.predict_proba(X_test)[:,1]\n",
    "submission_dnn = pd.DataFrame(data={'Opportunity_ID':X_test['Opportunity_ID'], 'Target': y_pred})\n",
    "submission_dnn = submission_dnn.groupby(\"Opportunity_ID\").agg({\"Target\":\"mean\"}).reset_index()\n",
    "submission_dnn.to_csv('../submits/dnn_with_woe_encoding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_featu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a35c532eda78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_best_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_featu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_best_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_featu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_featu' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_best_features = X_train.loc[:,best_featu]\n",
    "X_test_best_features = X_test.loc[:,best_featu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_best_train, x_best_validation, y_best_train, y_best_validation = train_test_split(X_train_best_features, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model_2 = dnn(x_best_train, y_best_train, x_best_validation, y_best_validation)\n",
    "test_model(dnn_model_2,x_best_validation,y_best_validation)\n",
    "cross_val(dnn_model_2, x_best_train, y_best_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = dnn_model_2.predict_proba(X_test_best_features)[:,1]\n",
    "submission_dnn_2 = pd.DataFrame(data={'Opportunity_ID':X_test_best_features['Opportunity_ID'], 'Target': y_pred_2})\n",
    "submission_dnn_2 = submission_dnn_2.groupby(\"Opportunity_ID\").agg({\"Target\":\"mean\"}).reset_index()\n",
    "submission_dnn_2.to_csv('../submits/dnn_best_features_with_woe_encoding.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
