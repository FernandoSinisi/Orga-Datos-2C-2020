{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers import  Dropout, Dense, Activation\n",
    "from tensorflow.keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../Feature_Engineering/data/other-cleaned_train.csv')\n",
    "test = pd.read_csv('../../Feature_Engineering/data/other-cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "test.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.copy()\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_columns = train.drop(columns = [\"Opportunity_ID\",\"ID\", \"Pricing, Delivery_Terms_Quote_Appr\",\\\n",
    "                                    \"Bureaucratic_Code_0_Approval\",\"Bureaucratic_Code_0_Approved\",\\\n",
    "                                    \"Submitted_for_Approval\",\"ASP\",\"ASP_(converted)\",\"TRF\",\"Total_Amount\",\\\n",
    "                                    \"Total_Taxable_Amount\",\"diferencia_en_dias\",\"Last_Modified_DOY\",\"Last_Modified_Year\",\\\n",
    "                                    \"Opportunity_Created_DOY\",\"Opportunity_Created_Year\",\"Quote_Expiry_DOY\",\\\n",
    "                                     \"Quote_Expiry_Year\",\"Planned_Delivery_Start_DOY\",\"Planned_Delivery_Start_Year\",\\\n",
    "                                    \"Planned_Delivery_End_DOY\",\"Planned_Delivery_End_Year\",\\\n",
    "                                    \"Target\"]).columns\n",
    "for column in categ_columns:\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(train[column], train['Target'])\n",
    "    feature_encoded = encoder.transform(train[column])\n",
    "    X_train = X_train.join(feature_encoded.add_suffix('_one_hot'))\n",
    "    X_train.drop(columns=[column], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_columns = test.drop(columns = [\"Opportunity_ID\",\"ID\", \"Pricing, Delivery_Terms_Quote_Appr\",\\\n",
    "                                    \"Bureaucratic_Code_0_Approval\",\"Bureaucratic_Code_0_Approved\",\\\n",
    "                                    \"Submitted_for_Approval\",\"ASP\",\"ASP_(converted)\",\"TRF\",\"Total_Amount\",\\\n",
    "                                    \"Total_Taxable_Amount\",\"diferencia_en_dias\",\"Last_Modified_DOY\",\"Last_Modified_Year\",\\\n",
    "                                    \"Opportunity_Created_DOY\",\"Opportunity_Created_Year\",\"Quote_Expiry_DOY\",\\\n",
    "                                     \"Quote_Expiry_Year\",\"Planned_Delivery_Start_DOY\",\"Planned_Delivery_Start_Year\",\\\n",
    "                                    \"Planned_Delivery_End_DOY\",\"Planned_Delivery_End_Year\"]).columns\n",
    "for column in categ_columns:\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(train[column], train['Target'])\n",
    "    feature_encoded = encoder.transform(test[column])\n",
    "    X_test = X_test.join(feature_encoded.add_suffix('_one_hot'))\n",
    "    X_test.drop(columns=[column], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"Total_Amount\"] = pd.to_numeric(X_train[\"Total_Amount\"],errors='coerce').fillna(X_train[\"Total_Amount\"].mean())\n",
    "X_train[\"Opportunity_Created_Year\"] = pd.to_numeric(X_train[\"Opportunity_Created_Year\"],errors='coerce').fillna(0)\n",
    "X_train[\"Quote_Expiry_DOY\"] = pd.to_numeric(X_train[\"Quote_Expiry_DOY\"],errors='coerce').fillna(0)\n",
    "X_train[\"Quote_Expiry_Year\"] = pd.to_numeric(X_train[\"Quote_Expiry_Year\"],errors='coerce').fillna(0)\n",
    "X_train[\"Planned_Delivery_End_DOY\"] = pd.to_numeric(X_train[\"Planned_Delivery_End_DOY\"],errors='coerce').fillna(0)\n",
    "X_train[\"Planned_Delivery_End_Year\"] = pd.to_numeric(X_train[\"Planned_Delivery_End_Year\"],errors='coerce').fillna(0)\n",
    "\n",
    "X_train = X_train.drop(columns = 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"Total_Amount\"] = pd.to_numeric(X_test[\"Total_Amount\"],errors='coerce').fillna(test[\"Total_Amount\"].mean())\n",
    "X_test[\"Opportunity_Created_Year\"] = pd.to_numeric(X_test[\"Opportunity_Created_Year\"],errors='coerce').fillna(0)\n",
    "X_test[\"Quote_Expiry_DOY\"] = pd.to_numeric(X_test[\"Quote_Expiry_DOY\"],errors='coerce').fillna(0)\n",
    "X_test[\"Quote_Expiry_Year\"] = pd.to_numeric(X_test[\"Quote_Expiry_Year\"],errors='coerce').fillna(0)\n",
    "X_test[\"Planned_Delivery_End_DOY\"] = pd.to_numeric(X_test[\"Planned_Delivery_End_DOY\"],errors='coerce').fillna(0)\n",
    "X_test[\"Planned_Delivery_End_Year\"] = pd.to_numeric(X_test[\"Planned_Delivery_End_Year\"],errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model, x_train, y_train):\n",
    "    score_cross_val = model_selection.cross_val_score(model, x_train, y_train, cv=5)\n",
    "    print(score_cross_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_model(optimizer='rmsprop',init='glorot_uniform'):\n",
    "    node = 512\n",
    "    nClasses = 2\n",
    "    dropout=0.5\n",
    "    nFeatures = 4307\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(node,input_dim=nFeatures,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,4):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(x_train, y_train, x_validation, y_validation):\n",
    "    keras_model = KerasClassifier(build_fn=DNN_model)\n",
    "    optimizers = ['rmsprop', 'adam']\n",
    "    init = ['glorot_uniform', 'normal', 'uniform']\n",
    "    epochs = [50, 100, 150]\n",
    "    batches = batches = [5, 10, 20]\n",
    "    param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init)\n",
    "    dnn_gs = GridSearchCV(keras_model, param_grid=param_grid)\n",
    "    dnn_gs.fit(x_train, y_train)\n",
    "    dnn_best = dnn_gs.best_estimator_\n",
    "    print(dnn_gs.best_params_)\n",
    "    print('dnn: {}'.format(dnn_best.score(x_validation, y_validation)))\n",
    "    return dnn_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x_test, y_test):\n",
    "    predictions = model.predict_proba(x_test)[:,1]\n",
    "    logloss = log_loss(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions.round())\n",
    "    print(\"Accuracy: %.2f%%, Logloss: %.2f\" % (accuracy*100.0, logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_features(model,train):\n",
    "    importance = model.feature_importances_\n",
    "    result = pd.DataFrame([train.columns,importance]).transpose()\n",
    "    result.columns = [\"Feature\",\"Importance\"]\n",
    "    return result.sort_values(by='Importance', ascending=False).head(15)[\"Feature\"].to_list()\n",
    "    \n",
    "    \n",
    "def plot_features(model,train):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(350, 350)\n",
    "    selection = SelectFromModel(model, threshold=0.040, prefit=True)\n",
    "    selected_dataset = selection.transform(train)\n",
    "    model.plot_importance(booster=model)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (40,20)\n",
    "    plt.xlabel(\"\\nFeature importance\", fontsize=40)\n",
    "    plt.ylabel(\"Features\", fontsize=35)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.Target\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(X_train, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891/1891 [==============================] - 9s 4ms/step - loss: 21418.6226 - accuracy: 0.5411\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 305.2404 - accuracy: 0.6701\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 27710.5135 - accuracy: 0.5332\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 508.9111 - accuracy: 0.6777\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 21950.1050 - accuracy: 0.5352\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 807.5694 - accuracy: 0.6743\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 20064.7830 - accuracy: 0.5163\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1392.0001 - accuracy: 0.6932\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 11469.4611 - accuracy: 0.5250\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1858.5231 - accuracy: 0.5565\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 17534.2648 - accuracy: 0.5428\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 156.9572 - accuracy: 0.6717\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 17562.0284 - accuracy: 0.5326\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 128.3142 - accuracy: 0.6810\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 25747.9560 - accuracy: 0.5152\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 764.3764 - accuracy: 0.5567\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 19339.5174 - accuracy: 0.5220\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 868.7886 - accuracy: 0.5637\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 20000.7245 - accuracy: 0.5229\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 522.1074 - accuracy: 0.5565\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 15945.6458 - accuracy: 0.5402\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 808.7662 - accuracy: 0.5584\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 32409.8402 - accuracy: 0.5291\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 2770.1062 - accuracy: 0.6777\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 14743.8659 - accuracy: 0.5246\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 247.7556 - accuracy: 0.6772\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 18803.5921 - accuracy: 0.5327\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 406.7194 - accuracy: 0.6627\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 20410.3461 - accuracy: 0.5321\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 2095.1467 - accuracy: 0.6653\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 22484.4497 - accuracy: 0.5421\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 326.9449 - accuracy: 0.6459\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 23048.7647 - accuracy: 0.5344\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 325.8509 - accuracy: 0.6789\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 17033.5607 - accuracy: 0.5373\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 40.9152 - accuracy: 0.6658\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 14985.4474 - accuracy: 0.5326\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5650\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 20142.6621 - accuracy: 0.5385\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 298.2368 - accuracy: 0.6674\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 25158.9139 - accuracy: 0.5319\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 130.6915 - accuracy: 0.6667\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 19447.8832 - accuracy: 0.5162\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 834.8970 - accuracy: 0.6806\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 24349.0999 - accuracy: 0.5348\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 740.8223 - accuracy: 0.6743\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 21524.3851 - accuracy: 0.5080\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 170.8046 - accuracy: 0.6708\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 13737.3839 - accuracy: 0.5266\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 5.9984 - accuracy: 0.5950\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 26432.9353 - accuracy: 0.5128\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 125.8913 - accuracy: 0.5698\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 15255.0509 - accuracy: 0.5271\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 964.3658 - accuracy: 0.6861\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 25379.4031 - accuracy: 0.5057\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 3695.3955 - accuracy: 0.6223\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 22921.3514 - accuracy: 0.5305\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 35.4576 - accuracy: 0.6877\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 19059.2590 - accuracy: 0.5230\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 781.6486 - accuracy: 0.6691\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 20355.1521 - accuracy: 0.5392\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 391.7898 - accuracy: 0.6159\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 12988.1865 - accuracy: 0.5416\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1138.0867 - accuracy: 0.6781\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 16882.6541 - accuracy: 0.5298\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 188.3235 - accuracy: 0.5567\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 25168.1026 - accuracy: 0.5265\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 644.9779 - accuracy: 0.6445\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 24864.8239 - accuracy: 0.5189\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1544.0833 - accuracy: 0.6280\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 24131.1625 - accuracy: 0.5251\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 41.3069 - accuracy: 0.6206\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 18452.6593 - accuracy: 0.5368\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 927.2486 - accuracy: 0.5728\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 26879.4709 - accuracy: 0.5129\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 388.1183 - accuracy: 0.6743\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 16251.9956 - accuracy: 0.5230\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1084.5259 - accuracy: 0.6509\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 24602.8714 - accuracy: 0.5137\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 844.1010 - accuracy: 0.5561\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 17104.0652 - accuracy: 0.5279\n",
      "473/473 [==============================] - 2s 2ms/step - loss: 1767.2882 - accuracy: 0.5715\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 19150.7103 - accuracy: 0.5160\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 3300.4863 - accuracy: 0.5728\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 29693.8564 - accuracy: 0.5250\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 2768.8816 - accuracy: 0.5567\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 16295.1893 - accuracy: 0.5339\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.7055 - accuracy: 0.5531\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 25638.8624 - accuracy: 0.5207\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 577.5200 - accuracy: 0.6644\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 14629.7453 - accuracy: 0.5244\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1170.9604 - accuracy: 0.5715\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 33865.3723 - accuracy: 0.5383\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 8.1002 - accuracy: 0.6066\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 15104.2856 - accuracy: 0.5198\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 171.0663 - accuracy: 0.5567\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 21262.5812 - accuracy: 0.5130\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1908.2992 - accuracy: 0.6293\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 19933.8922 - accuracy: 0.5292\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 110.7801 - accuracy: 0.6373\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 24491.1727 - accuracy: 0.5209\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 826.2205 - accuracy: 0.6282\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 24404.3948 - accuracy: 0.5178\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 511.8535 - accuracy: 0.6739\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 15546.9754 - accuracy: 0.5307\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.7017 - accuracy: 0.5635\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 32903.4948 - accuracy: 0.5150\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 67.4891 - accuracy: 0.6856\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 14667.5400 - accuracy: 0.5252\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 651.1205 - accuracy: 0.6648\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 21025.0849 - accuracy: 0.5420\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 189.0731 - accuracy: 0.6646\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 11367.4397 - accuracy: 0.5285\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.6703 - accuracy: 0.5969\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 24314.4171 - accuracy: 0.5332\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 6161.3164 - accuracy: 0.5567\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 11318.5364 - accuracy: 0.5343\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.5658\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 29374.5457 - accuracy: 0.5176\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1429.2264 - accuracy: 0.6712\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 17503.8375 - accuracy: 0.5411\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 25.8276 - accuracy: 0.6603\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 23069.4475 - accuracy: 0.5196\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 731.6210 - accuracy: 0.6620\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 17433.9301 - accuracy: 0.5137\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 232.2755 - accuracy: 0.6231\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 23095.6938 - accuracy: 0.5376\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 266.3591 - accuracy: 0.6496\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 19857.8120 - accuracy: 0.5278\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 418.2204 - accuracy: 0.6847\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 22212.0848 - accuracy: 0.5318\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 611.5703 - accuracy: 0.6375: 0s - los\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 17620.9336 - accuracy: 0.5325\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 447.0117 - accuracy: 0.6231\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 17087.5816 - accuracy: 0.5235\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 207.6664 - accuracy: 0.6430\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 15634.2399 - accuracy: 0.5230\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 60.9599 - accuracy: 0.6915\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 22588.1718 - accuracy: 0.5281\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1606.4695 - accuracy: 0.6411\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 27520.1805 - accuracy: 0.5313\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 439.2653 - accuracy: 0.6151\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 19651.8451 - accuracy: 0.5169\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 618.0395 - accuracy: 0.6629\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 15135.5974 - accuracy: 0.5041\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 202.9702 - accuracy: 0.6743\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 20132.8514 - accuracy: 0.5262\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 619.2075 - accuracy: 0.6966\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 20940.0892 - accuracy: 0.5039\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 658.8969 - accuracy: 0.6543\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 16953.9262 - accuracy: 0.5295\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 143.2063 - accuracy: 0.6667\n",
      "1891/1891 [==============================] - 10s 5ms/step - loss: 17314.6598 - accuracy: 0.5084\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 36.8828 - accuracy: 0.6827\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 25647.4513 - accuracy: 0.5229\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1170.0300 - accuracy: 0.6612\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 18601.3517 - accuracy: 0.5286\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 3212.7214 - accuracy: 0.5658\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 20203.1175 - accuracy: 0.5378\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 371.9259 - accuracy: 0.6792\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 16747.1128 - accuracy: 0.5144\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 2591.2083 - accuracy: 0.5715\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 31527.7888 - accuracy: 0.5311\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 2178.5413 - accuracy: 0.6417\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 13603.8976 - accuracy: 0.5418\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 22.6295 - accuracy: 0.5567\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 17032.0456 - accuracy: 0.5242\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 39.2477 - accuracy: 0.5658\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 30783.8907 - accuracy: 0.5260\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 1032.8881 - accuracy: 0.6703\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 22119.8512 - accuracy: 0.5200\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 8.0128 - accuracy: 0.6696\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 19046.7570 - accuracy: 0.5235\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 238.1636 - accuracy: 0.6751\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 14448.1301 - accuracy: 0.5357\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 672.4648 - accuracy: 0.5567\n",
      "1891/1891 [==============================] - 9s 5ms/step - loss: 26115.1849 - accuracy: 0.5258\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 485.9968 - accuracy: 0.6940\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 12224.6222 - accuracy: 0.5169\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 380.2116 - accuracy: 0.5565\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24847.1089 - accuracy: 0.5182\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 697.1177 - accuracy: 0.6717\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24827.6858 - accuracy: 0.5304\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 397.6500 - accuracy: 0.6624\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17134.9338 - accuracy: 0.5222\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 15.1179 - accuracy: 0.6595\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 15048.3065 - accuracy: 0.5305\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 58.4875 - accuracy: 0.6039\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 13626.2238 - accuracy: 0.5301\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 488.4519 - accuracy: 0.5341\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 16975.0696 - accuracy: 0.5207\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 349.5088 - accuracy: 0.5330\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 27559.1046 - accuracy: 0.5268\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 884.5267 - accuracy: 0.5499\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 14023.8602 - accuracy: 0.5350\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 342.2267 - accuracy: 0.6396\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 14657.1564 - accuracy: 0.5213\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 792.8433 - accuracy: 0.5662\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19594.9512 - accuracy: 0.5311\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1520.6829 - accuracy: 0.5713\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 22069.5518 - accuracy: 0.5236\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 740.2537 - accuracy: 0.6049\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 22536.7778 - accuracy: 0.5291\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 32.8519 - accuracy: 0.6108\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 16353.6778 - accuracy: 0.5183\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 850.3345 - accuracy: 0.6074\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19991.1345 - accuracy: 0.5104\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1304.1282 - accuracy: 0.6957\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 34713.8186 - accuracy: 0.5158\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1323.3004 - accuracy: 0.6293\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 28190.8253 - accuracy: 0.5220\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1516.5420 - accuracy: 0.6667\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 21853.6528 - accuracy: 0.5350\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 2305.5781 - accuracy: 0.4310\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17332.9929 - accuracy: 0.5312\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 669.2330 - accuracy: 0.5897\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19323.4314 - accuracy: 0.5132\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1233.3308 - accuracy: 0.5628\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 12718.4582 - accuracy: 0.5209\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 567.0887 - accuracy: 0.6674\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17836.7768 - accuracy: 0.5315\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 271.3272 - accuracy: 0.5706\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 29306.9148 - accuracy: 0.5144\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 5489.4932 - accuracy: 0.5563\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19451.0291 - accuracy: 0.4985\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 361.7139 - accuracy: 0.6206\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19252.6846 - accuracy: 0.5246\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 524.5369 - accuracy: 0.6813\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 14587.5624 - accuracy: 0.5384\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 207.7995 - accuracy: 0.6394\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24531.3847 - accuracy: 0.5410\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 903.9333 - accuracy: 0.5592\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 16883.9065 - accuracy: 0.5297\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 17.8178 - accuracy: 0.6256\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18160.8785 - accuracy: 0.5189\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 456.8682 - accuracy: 0.6168\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17071.1096 - accuracy: 0.5211\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1471.6191 - accuracy: 0.6098\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 28327.8818 - accuracy: 0.5187\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1476.7496 - accuracy: 0.6284\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18536.8763 - accuracy: 0.5283\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 3156.6731 - accuracy: 0.5715\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17155.0993 - accuracy: 0.5187\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 764.3719 - accuracy: 0.6650\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18229.5894 - accuracy: 0.5140\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 238.4051 - accuracy: 0.6772\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 23960.6707 - accuracy: 0.5243\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1632.7079 - accuracy: 0.4342\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 22390.6097 - accuracy: 0.5136\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 2946.4343 - accuracy: 0.6627\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 21151.2551 - accuracy: 0.5228\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 527.5858 - accuracy: 0.6629\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 22849.9291 - accuracy: 0.5339\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1235.5847 - accuracy: 0.5728\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24365.1187 - accuracy: 0.5264\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 781.5738 - accuracy: 0.6269\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 26632.5602 - accuracy: 0.5253\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 668.2007 - accuracy: 0.6221\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 15973.0170 - accuracy: 0.5232\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 3087.9080 - accuracy: 0.6035\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 20294.7933 - accuracy: 0.5189\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 393.0608 - accuracy: 0.6586\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24420.9674 - accuracy: 0.5183\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1127.7280 - accuracy: 0.6760\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 27960.4417 - accuracy: 0.5195\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 313.7987 - accuracy: 0.5567\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 20784.4914 - accuracy: 0.5294\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 102.1830 - accuracy: 0.6877\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18017.1654 - accuracy: 0.5239\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 766.9175 - accuracy: 0.6458\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 21799.6258 - accuracy: 0.5234\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 821.0314 - accuracy: 0.6210\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18379.0703 - accuracy: 0.5159\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 520.1087 - accuracy: 0.5728\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 26569.1213 - accuracy: 0.5426\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 707.9798 - accuracy: 0.5470\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 23719.2969 - accuracy: 0.5312\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 3043.4529 - accuracy: 0.6407\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 25868.6258 - accuracy: 0.5217\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 2123.0779 - accuracy: 0.5565\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 25312.7123 - accuracy: 0.5222\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 2002.4255 - accuracy: 0.5715\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 14600.8853 - accuracy: 0.5255\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 481.1142 - accuracy: 0.5178\n",
      "946/946 [==============================] - 5s 4ms/step - loss: 14291.0630 - accuracy: 0.5291\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1243.6528 - accuracy: 0.5833\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18960.8201 - accuracy: 0.5233\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1369.0538 - accuracy: 0.5353\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 20090.4032 - accuracy: 0.5179\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 707.2684 - accuracy: 0.6669\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 25272.9024 - accuracy: 0.5189\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 90.4603 - accuracy: 0.6646\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18493.3322 - accuracy: 0.5230\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1588.1511 - accuracy: 0.6366\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 21221.7141 - accuracy: 0.5271\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1110.9536 - accuracy: 0.6206\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18897.4089 - accuracy: 0.5099\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 198.2341 - accuracy: 0.6949\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18120.3935 - accuracy: 0.5351\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1255.9463 - accuracy: 0.6234\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19425.6915 - accuracy: 0.5314\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 84.9890 - accuracy: 0.6540\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17654.4926 - accuracy: 0.5098\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 159.7532 - accuracy: 0.6675\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17573.5502 - accuracy: 0.5230\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 2160.3972 - accuracy: 0.6332\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19258.7869 - accuracy: 0.5187\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 2553.0110 - accuracy: 0.5658\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 20902.8000 - accuracy: 0.5344\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1178.4233 - accuracy: 0.6149\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 21019.9753 - accuracy: 0.5207\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 2749.3486 - accuracy: 0.5550\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17497.5273 - accuracy: 0.5327\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 104.9883 - accuracy: 0.4962\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 20640.8113 - accuracy: 0.5165\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 19.8507 - accuracy: 0.5643\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24449.6443 - accuracy: 0.5053\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1241.3243 - accuracy: 0.5531\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24834.3084 - accuracy: 0.5389\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 749.1779 - accuracy: 0.6225\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24335.4965 - accuracy: 0.5225\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1494.5140 - accuracy: 0.5715\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 15630.5125 - accuracy: 0.5246\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 363.7541 - accuracy: 0.5893\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 27871.8397 - accuracy: 0.5223\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1260.4490 - accuracy: 0.4437\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 12845.8698 - accuracy: 0.5238\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 552.9925 - accuracy: 0.5658\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 10875.9454 - accuracy: 0.5122\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1727.9779 - accuracy: 0.6623\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 39473.2651 - accuracy: 0.5155\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 427.9696 - accuracy: 0.6667\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 18491.1784 - accuracy: 0.5098\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 937.4520 - accuracy: 0.6772\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19060.9831 - accuracy: 0.5385\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 355.4732 - accuracy: 0.5360\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 29039.9114 - accuracy: 0.5310\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 845.4915 - accuracy: 0.5658\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 24172.3031 - accuracy: 0.5308\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 162.4840 - accuracy: 0.6483\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17133.8019 - accuracy: 0.5425\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1460.3593 - accuracy: 0.5558\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 14896.5233 - accuracy: 0.5240\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 525.8308 - accuracy: 0.5656\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 37046.6102 - accuracy: 0.5261\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1168.1945 - accuracy: 0.5567\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 15809.1241 - accuracy: 0.5183\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 780.4758 - accuracy: 0.6881\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 27897.9699 - accuracy: 0.5233\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 3660.9988 - accuracy: 0.4672\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 19267.7683 - accuracy: 0.5349\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 264.1545 - accuracy: 0.6261\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 25130.4490 - accuracy: 0.5100\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 497.0238 - accuracy: 0.6827\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 21840.1656 - accuracy: 0.5173\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 147.9938 - accuracy: 0.5482\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 42340.0810 - accuracy: 0.5206\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 157.2278 - accuracy: 0.6949\n",
      "946/946 [==============================] - 5s 5ms/step - loss: 17027.2312 - accuracy: 0.5160\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 1304.8036 - accuracy: 0.6547\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 14887.9546 - accuracy: 0.5354\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 111.0326 - accuracy: 0.6514\n",
      "473/473 [==============================] - 6s 11ms/step - loss: 20509.5599 - accuracy: 0.5152\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2450.3872 - accuracy: 0.5863\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 19212.2072 - accuracy: 0.5077\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 807.7072 - accuracy: 0.6324\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 20368.5591 - accuracy: 0.5076\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 205.2542 - accuracy: 0.6974\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 27827.3630 - accuracy: 0.5133\n",
      "119/119 [==============================] - 1s 2ms/step - loss: 4425.5796 - accuracy: 0.5565\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 19934.9717 - accuracy: 0.5231\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 134.4979 - accuracy: 0.5715\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 16963.3528 - accuracy: 0.5314\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1670.0179 - accuracy: 0.5728\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 15355.2144 - accuracy: 0.5112\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1298.1573 - accuracy: 0.6747\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18502.8303 - accuracy: 0.5192\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1792.5663 - accuracy: 0.6809\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 26753.8341 - accuracy: 0.5152\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1571.1991 - accuracy: 0.5565\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 12790.0741 - accuracy: 0.5217\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 128.4462 - accuracy: 0.5512\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 28891.1034 - accuracy: 0.5198\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 173.8492 - accuracy: 0.6383\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 21755.2738 - accuracy: 0.5160\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1505.1864 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 22249.7843 - accuracy: 0.5168\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 446.3921 - accuracy: 0.6775\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17940.2501 - accuracy: 0.5108\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 672.3949 - accuracy: 0.6073\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 13828.5945 - accuracy: 0.5461\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 729.8147 - accuracy: 0.6620\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 15736.8511 - accuracy: 0.5159\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 976.1908 - accuracy: 0.6049\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17492.3278 - accuracy: 0.5278\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 891.7640 - accuracy: 0.5580\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 16244.2373 - accuracy: 0.5212\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2429.2144 - accuracy: 0.5658\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 22763.2719 - accuracy: 0.5177\n",
      "119/119 [==============================] - 1s 2ms/step - loss: 822.5759 - accuracy: 0.5408\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 31662.0599 - accuracy: 0.5191\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 832.6442 - accuracy: 0.6722\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 23772.6366 - accuracy: 0.5229\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 340.8810 - accuracy: 0.6747\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 29269.5209 - accuracy: 0.5228\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 583.4693 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18329.2614 - accuracy: 0.5201\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2001.8940 - accuracy: 0.5658\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 23850.7489 - accuracy: 0.5061\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 323.5365 - accuracy: 0.5565\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17024.0259 - accuracy: 0.5070\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 442.4938 - accuracy: 0.5745\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 26931.2446 - accuracy: 0.5164\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2706.7334 - accuracy: 0.6049\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 16515.2789 - accuracy: 0.5070\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2057.4731 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18506.1418 - accuracy: 0.5385\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1809.2937 - accuracy: 0.5569\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 23561.4305 - accuracy: 0.5293\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 620.5411 - accuracy: 0.6661\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 20091.6276 - accuracy: 0.5283\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 3247.1765 - accuracy: 0.5715\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 15502.7920 - accuracy: 0.5269\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1608.4838 - accuracy: 0.5728\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 15916.8276 - accuracy: 0.5225\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 681.0302 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 20849.3407 - accuracy: 0.5165\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 532.7433 - accuracy: 0.6949\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 21050.8573 - accuracy: 0.5300\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 3206.1104 - accuracy: 0.6394\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 29534.9143 - accuracy: 0.5253\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1061.0127 - accuracy: 0.5715\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 16175.8935 - accuracy: 0.5212\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 287.4636 - accuracy: 0.6819\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 19378.0341 - accuracy: 0.5034\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1223.5214 - accuracy: 0.6125\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 22221.7713 - accuracy: 0.5211\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2638.5642 - accuracy: 0.6140\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 14995.7541 - accuracy: 0.5196\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1441.9910 - accuracy: 0.6344\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 13106.1244 - accuracy: 0.5305\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 671.3164 - accuracy: 0.5715\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 19241.6966 - accuracy: 0.5219\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 3831.1848 - accuracy: 0.5728\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17385.2910 - accuracy: 0.5260\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 464.2572 - accuracy: 0.4911\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 21100.3288 - accuracy: 0.5200\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 132.6326 - accuracy: 0.6953\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 23360.0613 - accuracy: 0.5131\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1132.3058 - accuracy: 0.5565\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17520.2057 - accuracy: 0.5310\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2531.2375 - accuracy: 0.5605\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 15912.4910 - accuracy: 0.5177\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 165.0983 - accuracy: 0.6734\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17054.5098 - accuracy: 0.5100\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1622.3872 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 22495.1995 - accuracy: 0.5020\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1325.4354 - accuracy: 0.6970\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17679.3495 - accuracy: 0.5275\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 253.0192 - accuracy: 0.6437\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17557.8309 - accuracy: 0.5109\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2621.7397 - accuracy: 0.5719\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 21483.5971 - accuracy: 0.5148\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 185.1807 - accuracy: 0.6662\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 16051.3270 - accuracy: 0.5159\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1815.3040 - accuracy: 0.5859\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 13758.3210 - accuracy: 0.5176\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1857.6985 - accuracy: 0.5967\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 15467.8582 - accuracy: 0.5279\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 215.2155 - accuracy: 0.6318\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 25498.3160 - accuracy: 0.5378\n",
      "119/119 [==============================] - 1s 2ms/step - loss: 301.2221 - accuracy: 0.6607\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 15039.8645 - accuracy: 0.5087\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1461.1587 - accuracy: 0.5728\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 23016.5787 - accuracy: 0.5208\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 712.4349 - accuracy: 0.5867\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 17690.9529 - accuracy: 0.5120\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2162.2507 - accuracy: 0.6504\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18504.0364 - accuracy: 0.5152\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 454.4258 - accuracy: 0.6631\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18433.2909 - accuracy: 0.5129\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 136.8339 - accuracy: 0.6430\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 22675.2023 - accuracy: 0.5263\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1132.5985 - accuracy: 0.5330\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 16990.0097 - accuracy: 0.5145\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2727.9548 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 22925.0490 - accuracy: 0.5217\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 296.6946 - accuracy: 0.5658\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 19196.7158 - accuracy: 0.5263\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 374.0142 - accuracy: 0.6644\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 19942.1145 - accuracy: 0.5238\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 698.2953 - accuracy: 0.6083\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18996.7033 - accuracy: 0.5273\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2023.4070 - accuracy: 0.6764\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18937.2265 - accuracy: 0.5183\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 981.6930 - accuracy: 0.6307\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 31471.5752 - accuracy: 0.5209\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 502.0294 - accuracy: 0.6653\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 25564.1536 - accuracy: 0.5093\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 304.1095 - accuracy: 0.6703\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 26150.5483 - accuracy: 0.5173\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1338.4117 - accuracy: 0.5203\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18812.3382 - accuracy: 0.5240\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1511.5348 - accuracy: 0.5728\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 24685.9503 - accuracy: 0.5103\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 396.5314 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 18778.9406 - accuracy: 0.5297\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1958.1146 - accuracy: 0.6043\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 21071.6582 - accuracy: 0.5196\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 833.6596 - accuracy: 0.5565\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 25768.3304 - accuracy: 0.5247\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2398.9170 - accuracy: 0.4501\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 20992.6501 - accuracy: 0.5227\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1337.7744 - accuracy: 0.5884\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 24001.8334 - accuracy: 0.5183\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 384.0534 - accuracy: 0.6772\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 23630.3996 - accuracy: 0.5280\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 359.5770 - accuracy: 0.6940\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 22631.5585 - accuracy: 0.5289\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 230.1663 - accuracy: 0.5565\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 26208.5218 - accuracy: 0.5067\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1462.6814 - accuracy: 0.6151\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 21494.6386 - accuracy: 0.5161\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2418.7773 - accuracy: 0.5728\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 20015.1574 - accuracy: 0.5147\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2923.6523 - accuracy: 0.5406\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 22421.4367 - accuracy: 0.4965\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 3940.0225 - accuracy: 0.5400\n",
      "473/473 [==============================] - 5s 9ms/step - loss: 15667.2973 - accuracy: 0.5287\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1290.5142 - accuracy: 0.6504\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 28897.7412 - accuracy: 0.5209\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2514.4609 - accuracy: 0.6667\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 23957.5921 - accuracy: 0.5305\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 1512.7516 - accuracy: 0.6134\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 33813.5950 - accuracy: 0.5194\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2004.9858 - accuracy: 0.5567\n",
      "473/473 [==============================] - 5s 11ms/step - loss: 14323.4586 - accuracy: 0.5146\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 3451.7612 - accuracy: 0.5967\n",
      "473/473 [==============================] - 5s 10ms/step - loss: 15191.1426 - accuracy: 0.5299\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 857.3671 - accuracy: 0.5523\n",
      "2364/2364 [==============================] - 11s 4ms/step - loss: 17414.4082 - accuracy: 0.5382\n",
      "{'batch_size': 5, 'init': 'uniform', 'nb_epoch': 100, 'optimizer': 'rmsprop'}\n",
      "1013/1013 [==============================] - 2s 2ms/step - loss: 206.7369 - accuracy: 0.5747\n",
      "dnn: 0.5747285485267639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.47%, Logloss: nan\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 16122.3236 - accuracy: 0.5237\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 6.1794 - accuracy: 0.6117\n",
      "1891/1891 [==============================] - 9s 4ms/step - loss: 20529.4841 - accuracy: 0.5323\n",
      "473/473 [==============================] - 1s 2ms/step - loss: 3017.3506 - accuracy: 0.5728\n",
      "1774/1891 [===========================>..] - ETA: 0s - loss: 15770.5701 - accuracy: 0.5544"
     ]
    }
   ],
   "source": [
    "dnn_model = dnn(x_train, y_train, x_validation, y_validation)\n",
    "test_model(dnn_model,x_validation,y_validation)\n",
    "cross_val(dnn_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_model.predict_proba(X_test)[:,1]\n",
    "submission_dnn = pd.DataFrame(data={'Opportunity_ID':X_test['Opportunity_ID'], 'Target': y_pred})\n",
    "submission_dnn = submission_dnn.groupby(\"Opportunity_ID\").agg({\"Target\":\"mean\"}).reset_index()\n",
    "submission_dnn.to_csv('../submits/dnn_with_one_hot_encoding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_featu = best_features(dnn_model,X_train)\n",
    "if \"Opportunity_ID\" not in best_featu: \n",
    "    best_featu.append(\"Opportunity_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_best_features = X_train.loc[:,best_featu]\n",
    "X_test_best_features = X_test.loc[:,best_featu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_best_train, x_best_validation, y_best_train, y_best_validation = train_test_split(X_train_best_features, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model_2 = dnn(x_best_train, y_best_train, x_best_validation, y_best_validation)\n",
    "test_model(dnn_model_2,x_best_validation,y_best_validation)\n",
    "cross_val(dnn_model_2, x_best_train, y_best_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = dnn_model_2.predict_proba(X_test_best_features)[:,1]\n",
    "submission_dnn_2 = pd.DataFrame(data={'Opportunity_ID':X_test_best_features['Opportunity_ID'], 'Target': y_pred_2})\n",
    "submission_dnn_2 = submission_dnn_2.groupby(\"Opportunity_ID\").agg({\"Target\":\"mean\"}).reset_index()\n",
    "submission_dnn_2.to_csv('../submits/dnn_best_features_with_one_hot_encoding.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
